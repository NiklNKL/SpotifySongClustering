{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_and_concatenate_parquet_files(folder_path):\n",
    "    # List all files in the directory\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith('.parquet')]\n",
    "    \n",
    "    # Sort files for consistent order if needed (optional)\n",
    "    files.sort()\n",
    "\n",
    "    # Load and concatenate all the Parquet files\n",
    "    df_list = []\n",
    "    for file in files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_parquet(file_path)\n",
    "        df_list.append(df)\n",
    "\n",
    "    # Concatenate all dataframes\n",
    "    concatenated_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    return concatenated_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "import math\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "def save_dataframe_as_parquet(df:pd.DataFrame, folder_path=\"data\", folder_name=None, always_overwrite=None, model_object=None):\n",
    "    if not folder_name:\n",
    "        folder_name = input(\"Enter the name of the folder to save the files: \")\n",
    "\n",
    "    full_path = os.path.join(folder_path, folder_name)\n",
    "\n",
    "    # Check if the folder already exists\n",
    "    if os.path.exists(full_path) and always_overwrite is not True:\n",
    "        if always_overwrite is None:\n",
    "            overwrite = input(f\"The folder '{folder_name}' already exists. Do you want to overwrite it? (yes/no): \")\n",
    "            always_overwrite = overwrite.lower() != 'yes'\n",
    "        if not always_overwrite:\n",
    "            suffix = 1\n",
    "            new_folder_name = f\"{folder_name}_{suffix}\"\n",
    "            while os.path.exists(os.path.join(folder_path, new_folder_name)):\n",
    "                suffix += 1\n",
    "                new_folder_name = f\"{folder_name}_{suffix}\"\n",
    "            folder_name = new_folder_name\n",
    "            full_path = os.path.join(folder_path, folder_name)\n",
    "    \n",
    "    os.makedirs(full_path, exist_ok=True)\n",
    "\n",
    "    temp_file = os.path.join(full_path, \"temp.parquet\")\n",
    "    df.to_parquet(temp_file)\n",
    "    file_size = os.path.getsize(temp_file) / (1024 * 1024)  # Size in MB\n",
    "    os.remove(temp_file)\n",
    "\n",
    "    if file_size > 50:\n",
    "        num_splits = math.ceil(file_size / 50)\n",
    "        row_split = math.ceil(len(df) / num_splits)\n",
    "    else:\n",
    "        num_splits = 1\n",
    "        row_split = len(df)\n",
    "    \n",
    "    for i in range(num_splits):\n",
    "        start_row = i * row_split\n",
    "        end_row = min((i + 1) * row_split, len(df))\n",
    "        split_df = df.iloc[start_row:end_row]\n",
    "        split_file_name = os.path.join(full_path, f\"{folder_name}_part_{i + 1}.parquet\")\n",
    "        split_df.to_parquet(split_file_name)\n",
    "    \n",
    "    if model_object:\n",
    "        for key, value in model_object.items():\n",
    "            joblib.dump(value, f'{full_path}/{key}_model.pkl')\n",
    "\n",
    "    print(f\"Dataframe saved in {num_splits} files under the folder: {full_path}\")\n",
    "    \n",
    "    return full_path\n",
    "\n",
    "# Example usage:\n",
    "# save_dataframe_as_parquet(df=kmeans_cluster, folder_path=\"data\", folder_name=\"kmeans_clustered_subset\", always_overwrite=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "original_data = load_and_concatenate_parquet_files(\"data/spotify_data\")\n",
    "\n",
    "original_data[\"year\"] = pd.to_datetime(original_data[\"year\"], format='%Y')\n",
    "original_data = original_data.dropna(subset=[\"danceability\", \"energy\", \"key\", \"loudness\", \"mode\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\", \"duration_ms\", \"time_signature\", \"popularity\", \"track_id\", \"track_name\", \"artist_name\", \"year\"])\n",
    "original_data = original_data.drop_duplicates(subset=[\"track_name\", \"artist_name\", \"danceability\", \"energy\", \"key\", \"mode\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\", \"time_signature\"])\n",
    "original_data = original_data.reset_index(drop=True)\n",
    "original_data = original_data.drop(columns=[\"Unnamed: 0\"])\n",
    "original_data[\"year\"] = pd.to_datetime(original_data[\"year\"], format='%Y')\n",
    "original_data['year'] = original_data['year'].dt.year\n",
    "original_data[\"popularity\"] = original_data[\"popularity\"].astype(int)\n",
    "save_dataframe_as_parquet(original_data, folder_name=\"preprocessed_spotify_data\")\n",
    "display(original_data.head())\n",
    "display(original_data.describe())\n",
    "print(original_data.info())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Clustering Spotify Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis for Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Data Scatter Plot with Genre as color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_utils import load_and_concatenate_parquet_files\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly_express as px\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "preprocessed_data = load_and_concatenate_parquet_files(\"data/preprocessed_spotify_data\")\n",
    "\n",
    "metric_columns = [\"danceability\", \"energy\", \"loudness\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\"]\n",
    "\n",
    "data_for_analysis = preprocessed_data.copy()\n",
    "data_for_analysis = data_for_analysis[data_for_analysis[\"genre\"].notna()]\n",
    "data_for_analysis = data_for_analysis.sample(n=200000, random_state=42)\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data_for_analysis[metric_columns])\n",
    "scaled_data_for_analysis = pd.merge(pd.DataFrame(data_scaled, columns=metric_columns), data_for_analysis.drop(columns=metric_columns), left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(scaled_data_for_analysis[metric_columns])\n",
    "scaled_data_for_analysis[[\"PCA1\", \"PCA2\"]] = pca.transform(scaled_data_for_analysis[metric_columns])\n",
    "\n",
    "px.scatter(scaled_data_for_analysis, x=\"PCA1\", y=\"PCA2\", color=\"genre\", hover_data={'PCA1': False, 'PCA2': False, 'genre': True, 'artist_name': True, 'track_name': True, \n",
    "                        'danceability': True, 'energy': True, 'valence': True},).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that genres cannot easily be identified by music features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters with HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_utils import load_and_concatenate_parquet_files\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly_express as px\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "hdbscan_cluster_df = load_and_concatenate_parquet_files(\"data/hdbscan_clustered_subset\")\n",
    "\n",
    "metric_columns = [\"danceability\", \"energy\", \"loudness\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\"]\n",
    "\n",
    "data_for_analysis = hdbscan_cluster_df.copy()\n",
    "sample_size = 200000 if len(data_for_analysis) > 200000 else len(data_for_analysis)\n",
    "data_for_analysis = data_for_analysis.sample(n=sample_size, random_state=42)\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data_for_analysis[metric_columns])\n",
    "scaled_data_for_analysis = pd.merge(pd.DataFrame(data_scaled, columns=metric_columns), data_for_analysis.drop(columns=metric_columns), left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(scaled_data_for_analysis[metric_columns])\n",
    "scaled_data_for_analysis[[\"PCA1\", \"PCA2\"]] = pca.transform(scaled_data_for_analysis[metric_columns])\n",
    "\n",
    "\n",
    "px.scatter(scaled_data_for_analysis, x=\"PCA1\", y=\"PCA2\", color=\"hdbscan_cluster\", hover_data={'PCA1': False, 'PCA2': False, 'genre': True, 'artist_name': True, 'track_name': True, \n",
    "                        'danceability': True, 'energy': True, 'valence': True},).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hdbscan_cluster_df[\"hdbscan_cluster\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters with KMEANS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from custom_utils import load_and_concatenate_parquet_files, save_dataframe_as_parquet\n",
    "\n",
    "def calculate_silhouette_score():\n",
    "    preprocessed_data = load_and_concatenate_parquet_files(\"data/preprocessed_spotify_data\")\n",
    "    metric_columns = [\"danceability\", \"energy\", \"loudness\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\"]\n",
    "\n",
    "    data_for_analysis = preprocessed_data.copy()\n",
    "    data_for_analysis = data_for_analysis.sample(n=20000, random_state=42)\n",
    "\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = scaler.fit_transform(data_for_analysis[metric_columns])\n",
    "\n",
    "    k_range = range(2, 11)\n",
    "    silhouette_scores = []\n",
    "\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        cluster_labels = kmeans.fit_predict(data_scaled) \n",
    "        score = silhouette_score(data_scaled, cluster_labels)\n",
    "        silhouette_scores.append(score)\n",
    "        print(f\"Number of clusters: {k}, Silhouette Score: {score}\")\n",
    "\n",
    "    silhouette_df = pd.DataFrame({\n",
    "        'Number of Clusters (k)': k_range,\n",
    "        'Silhouette Score': silhouette_scores\n",
    "    })\n",
    "\n",
    "    save_dataframe_as_parquet(silhouette_df, \"data\", \"silhouette_scores\", always_overwrite=True)\n",
    "\n",
    "# calculate_silhouette_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_utils import load_and_concatenate_parquet_files\n",
    "\n",
    "silhouette_scores = load_and_concatenate_parquet_files(\"data/silhouette_scores\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(silhouette_scores['Number of Clusters (k)'], silhouette_scores['Silhouette Score'], marker='o')\n",
    "plt.title('Silhouette Scores for Different Number of Clusters')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.xticks(silhouette_scores['Number of Clusters (k)'])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from custom_utils import load_and_concatenate_parquet_files, save_dataframe_as_parquet\n",
    "\n",
    "def calculate_elbow_score():\n",
    "    preprocessed_data = load_and_concatenate_parquet_files(\"data/preprocessed_spotify_data\")\n",
    "    metric_columns = [\"danceability\", \"energy\", \"loudness\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\"]\n",
    "\n",
    "    data_for_analysis = preprocessed_data.copy()\n",
    "    data_for_analysis = data_for_analysis.sample(n=20000, random_state=42)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = scaler.fit_transform(data_for_analysis[metric_columns])\n",
    "\n",
    "    k_range = range(2, 30)\n",
    "    wcss = []\n",
    "\n",
    "\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(data_scaled) \n",
    "        wcss.append(kmeans.inertia_) \n",
    "        print(f\"Number of clusters: {k}, WCSS: {kmeans.inertia_}\")\n",
    "\n",
    "    wcss_df = pd.DataFrame({\n",
    "        'Number of Clusters (k)': k_range,\n",
    "        'Elbow Method Score': wcss\n",
    "    })\n",
    "\n",
    "    save_dataframe_as_parquet(wcss_df, \"data\", \"elbow_method_scores\", always_overwrite=True)\n",
    "\n",
    "# calculate_elbow_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_utils import load_and_concatenate_parquet_files\n",
    "\n",
    "elbow_method_scores = load_and_concatenate_parquet_files(\"data/elbow_method_scores\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(elbow_method_scores['Number of Clusters (k)'], elbow_method_scores['Elbow Method Score'], marker='o')\n",
    "plt.title('Elbow Method Scores for Different Number of Clusters')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Elbow Method Score')\n",
    "plt.xticks(elbow_method_scores['Number of Clusters (k)'])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting KMeans Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_utils import load_and_concatenate_parquet_files\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly_express as px\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "kmeans_cluster_df = load_and_concatenate_parquet_files(\"data/kmeans_clustered_subset\")\n",
    "\n",
    "metric_columns = [\"danceability\", \"energy\", \"loudness\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\"]\n",
    "\n",
    "data_for_analysis = kmeans_cluster_df.copy()\n",
    "data_for_analysis = data_for_analysis.sample(n=200000, random_state=42)\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data_for_analysis[metric_columns])\n",
    "scaled_data_for_analysis = pd.merge(pd.DataFrame(data_scaled, columns=metric_columns), data_for_analysis.drop(columns=metric_columns), left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(scaled_data_for_analysis[metric_columns])\n",
    "scaled_data_for_analysis[[\"PCA1\", \"PCA2\"]] = pca.transform(scaled_data_for_analysis[metric_columns])\n",
    "\n",
    "\n",
    "px.scatter(scaled_data_for_analysis, x=\"PCA1\", y=\"PCA2\", color=\"kmeans_cluster\", hover_data={'PCA1': False, 'PCA2': False, 'genre': True, 'artist_name': True, 'track_name': True, \n",
    "                        'danceability': True, 'energy': True, 'valence': True},).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans_cluster_df[\"kmeans_cluster\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster with KMEANS and HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from custom_utils import load_and_concatenate_parquet_files\n",
    "\n",
    "kmeans_hdbscan_cluster_df = load_and_concatenate_parquet_files(\"data/kmeans_hdbscan_clustered_subset\")\n",
    "metric_columns = [\"danceability\", \"energy\", \"loudness\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\"]\n",
    "\n",
    "data_for_analysis = kmeans_hdbscan_cluster_df.copy()\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data_for_analysis[metric_columns])\n",
    "scaled_data_for_analysis = pd.merge(pd.DataFrame(data_scaled, columns=metric_columns), \n",
    "                                    data_for_analysis.drop(columns=metric_columns), \n",
    "                                    left_index=True, right_index=True)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(data_scaled)\n",
    "scaled_data_for_analysis[[\"PCA1\", \"PCA2\"]] = pca_result\n",
    "\n",
    "kmeans_clusters = data_for_analysis['kmeans_cluster'].unique()\n",
    "num_clusters = len(kmeans_clusters)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=int(np.ceil(num_clusters / 2)), ncols=2, figsize=(15, 5 * np.ceil(num_clusters / 2)))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, cluster in enumerate(kmeans_clusters):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    cluster_data = scaled_data_for_analysis[scaled_data_for_analysis['kmeans_cluster'] == cluster]\n",
    "    cluster_data_sampled = cluster_data.sample(frac=0.1, random_state=42)\n",
    "    \n",
    "    scatter = ax.scatter(cluster_data_sampled['PCA1'], cluster_data_sampled['PCA2'], \n",
    "                        c=cluster_data_sampled['hdbscan_cluster'], cmap='tab20', alpha=0.5, s=10)\n",
    "    ax.set_title(f'K-Means Cluster {cluster}')\n",
    "    ax.set_xlabel('PCA1')\n",
    "    ax.set_ylabel('PCA2')\n",
    "    \n",
    "    cbar = plt.colorbar(scatter, ax=ax, orientation='vertical')\n",
    "    cbar.set_label('HDBSCAN Cluster')\n",
    "\n",
    "for j in range(num_clusters, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_kmeans_hdbscan_cluster_df = kmeans_hdbscan_cluster_df.copy()\n",
    "new_kmeans_hdbscan_cluster_df[\"combined_cluster\"] = new_kmeans_hdbscan_cluster_df[\"kmeans_cluster\"].astype(str) + \"_\" + new_kmeans_hdbscan_cluster_df[\"hdbscan_cluster\"].astype(str)\n",
    "print(new_kmeans_hdbscan_cluster_df[\"combined_cluster\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_utils import load_and_concatenate_parquet_files, save_dataframe_as_parquet\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import hdbscan\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "\n",
    "def reduce_data(data, dimensions, metric_columns):\n",
    "    numeric_columns = data[metric_columns]\n",
    "    pca_standardized = PCA(n_components=dimensions)\n",
    "    pca_standardized_result = pca_standardized.fit_transform(numeric_columns)\n",
    "    return pca_standardized_result\n",
    "\n",
    "def normalize_data(data:pd.DataFrame, metric_columns):\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    numeric_columns = data[metric_columns]\n",
    "    other_columns = data.drop(columns=metric_columns).reset_index(drop=True)\n",
    "    normalized_data = min_max_scaler.fit_transform(numeric_columns)\n",
    "    normalized_df = pd.DataFrame(normalized_data, columns=numeric_columns.columns)\n",
    "    normalized_data = pd.merge(normalized_df, other_columns, left_index=True, right_index=True)\n",
    "    return normalized_df\n",
    "\n",
    "def cluster_with_kmeans(data_for_clustering:pd.DataFrame):\n",
    "    kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "    kmeans_labels = kmeans.fit_predict(data_for_clustering)\n",
    "    return kmeans_labels, kmeans\n",
    "\n",
    "def cluster_with_hdbscan(data_for_clustering:pd.DataFrame):\n",
    "    hdbscan_clusterer = hdbscan.HDBSCAN(min_cluster_size=10, prediction_data=True)\n",
    "    hdbscan_labels = hdbscan_clusterer.fit_predict(data_for_clustering)\n",
    "    return hdbscan_labels, hdbscan_clusterer\n",
    "\n",
    "def run_kmeans_and_hdbscan(original_data:pd.DataFrame, cluster_data:pd.DataFrame):\n",
    "    data_for_clustering = cluster_data.copy()\n",
    "    kmeans_labels, kmeans = cluster_with_kmeans(data_for_clustering)\n",
    "\n",
    "    data_for_clustering[\"kmeans_cluster\"] = kmeans_labels\n",
    "    original_data[\"kmeans_cluster\"] = kmeans_labels\n",
    "    print(f\"KMeans_cluster distro: {data_for_clustering['kmeans_cluster'].value_counts()}\")\n",
    "    hdbscan_cluster_df = []\n",
    "    hdbscan_models = {}\n",
    "    for cluster, data in data_for_clustering.groupby(\"kmeans_cluster\"):\n",
    "        hdbscan_cluster_labels, hdbscan_clusterer = cluster_with_hdbscan(data.drop(columns=[\"kmeans_cluster\"]))\n",
    "        current_original_data = original_data[original_data[\"kmeans_cluster\"] == cluster].reset_index(drop=True).copy()\n",
    "        current_original_data[\"hdbscan_cluster\"] = hdbscan_cluster_labels\n",
    "        hdbscan_cluster_df.append(current_original_data)\n",
    "        hdbscan_models[cluster] = hdbscan_clusterer\n",
    "        print(f\"HDBSCAN_cluster distro for kmeans {cluster}: {current_original_data['hdbscan_cluster'].value_counts()}\")\n",
    "\n",
    "    all_cluster_results = pd.concat(hdbscan_cluster_df).reset_index(drop=True)\n",
    "    \n",
    "    return all_cluster_results, kmeans, hdbscan_models\n",
    "\n",
    "def cluster_data(original_data:pd.DataFrame, subset_fraction=None, kmeans=True, use_hdbscan=True, should_normalize_data=True, pca_dimensions=None, columns_to_use=None):\n",
    "    \n",
    "    if subset_fraction:\n",
    "        original_data = original_data.sample(frac=subset_fraction)\n",
    "    \n",
    "    data_for_clustering = original_data.copy().reset_index(drop=True)\n",
    "    \n",
    "    metric_columns = columns_to_use if columns_to_use else [\"danceability\", \"energy\", \"key\", \"loudness\", \"mode\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\", \"time_signature\"]\n",
    "    \n",
    "    if should_normalize_data:\n",
    "        data_for_clustering = normalize_data(data_for_clustering, metric_columns=metric_columns)\n",
    "\n",
    "    if pca_dimensions:\n",
    "        data_for_clustering = reduce_data(data_for_clustering, pca_dimensions, metric_columns)\n",
    "    \n",
    "    if pca_dimensions is None:\n",
    "        data_for_clustering = data_for_clustering[metric_columns]\n",
    "\n",
    "    if kmeans and use_hdbscan:\n",
    "        result_df, kmeans, hdbscan_models = run_kmeans_and_hdbscan(original_data, data_for_clustering)\n",
    "        folder_path = save_dataframe_as_parquet(result_df, folder_path=\"data\", folder_name=\"kmeans_hdbscan_clustered_subset\", always_overwrite=False, model_object={\"kmeans\": kmeans})\n",
    "        for key, value in hdbscan_models.items():\n",
    "            joblib.dump(value, f'{folder_path}/hdbscan_model_{key}.pkl')\n",
    "        result = result_df.copy()\n",
    "    \n",
    "    elif kmeans:\n",
    "        kmeans_labels, kmeans = cluster_with_kmeans(data_for_clustering)\n",
    "        original_data[\"kmeans_cluster\"] = kmeans_labels\n",
    "        save_dataframe_as_parquet(original_data, folder_path=\"data\", folder_name=\"kmeans_clustered_subset\", always_overwrite=False, model_object={\"kmeans\": kmeans})\n",
    "        print(f\"KMeans_cluster distro: {original_data['kmeans_cluster'].value_counts()}\")\n",
    "        result = original_data.copy()\n",
    "        \n",
    "    elif use_hdbscan:\n",
    "        hdbscan_labels, hdbscan_clusterer = cluster_with_hdbscan(data_for_clustering)\n",
    "        original_data[\"hdbscan_cluster\"] = hdbscan_labels\n",
    "        save_dataframe_as_parquet(original_data, folder_path=\"data\", folder_name=\"hdbscan_clustered_subset\", always_overwrite=False, model_object={\"hdbscan\": hdbscan_clusterer})\n",
    "        print(f\"HDBSCAN_cluster distro: {original_data['hdbscan_cluster'].value_counts()}\")\n",
    "        result = original_data.copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "# preprocessed_data = load_and_concatenate_parquet_files(\"data/preprocessed_spotify_data\")\n",
    "# clustered_data = cluster_data(preprocessed_data, subset_fraction=0.02, kmeans=False, use_hdbscan=True,  should_normalize_data=True, pca_dimensions=None, columns_to_use=None)\n",
    "# display(clustered_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

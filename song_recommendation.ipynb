{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "- https://www.kaggle.com/datasets/notshrirang/spotify-million-song-dataset\n",
    "- https://www.kaggle.com/datasets/tonygordonjr/spotify-dataset-2023?select=spotify-albums_data_2023.csv\n",
    "- https://www.kaggle.com/datasets/yamaerenay/spotify-dataset-19212020-600k-tracks?select=tracks.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links\n",
    "- https://forecastegy.com/posts/xgboost-multiclass-classification-python/\n",
    "- https://github.com/jannine92/spotify_recommendation/blob/main/music_recommender.ipynb\n",
    "- https://www.kaggle.com/code/nyjoey/spotify-clustering\n",
    "- https://ausaf-a.github.io/ml-song-recommender/\n",
    "- https://medium.com/@Marlon_H/spotify-clustering-f41b40003c9a\n",
    "- https://www.kaggle.com/code/choongqianzheng/song-genre-classification-system\n",
    "- https://developer.spotify.com/documentation/web-api/reference/get-audio-features\n",
    "- https://medium.com/@miguelrodrigueznovelo/discover-your-perfect-playlist-10-songs-recommended-by-a-music-recommendation-system-with-python-5fd246d87127\n",
    "- https://medium.com/@shruti.somankar/building-a-music-recommendation-system-using-spotify-api-and-python-f7418a21fa41\n",
    "- https://www.kaggle.com/code/merveeyuboglu/music-recommendation-system-cosine-s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo List:\n",
    "\n",
    "- Basic stuff✅\n",
    "  - Load Data✅\n",
    "  -  Display, Info and Describe data✅\n",
    "  - Split Datasets into song_metrics and song_info✅\n",
    "- Data Visualization (Also in Percent if valuable)✅\n",
    "  - Visualize Correlation Heatmap✅\n",
    "  - Display Genres as Numbers and Histogram✅\n",
    "  - Display Genre Dendogram✅\n",
    "  - Display most frequent artists✅\n",
    "  - Display most popular artist✅\n",
    "  - Plot Popularity as histogram✅\n",
    "  - Plot Average Song metric for Each genre (could also be on a 3D plot)✅\n",
    "  - Plot Box plots to detect outliers✅\n",
    "- Features\n",
    "  - Apply Standard and MinMaxScaler ✅\n",
    "  - Apply and Visualize PCA and t-SNE / UMAP\n",
    "  - Use Silhouette Score to see how many clusters are needed (also try fancy plot from Medium)\n",
    "  - Use KMeans to start\n",
    "  - Use DBSCAN\n",
    "  - Use Agglomerative Clustering\n",
    "  - Use HDBSCAN\n",
    "  - Use XGBClassifier with Cross Validation\n",
    "- Feature Extensions\n",
    "  - Plot Similar Artists\n",
    "  - Plot Similar Genres\n",
    "  - (Plot Similar Songs [Only small set of Data here])\n",
    "- Possible Uses:\n",
    "  - Put song into spotify api, get song data back, and use that to find similar songs (with possibility to get different artists than the one from the provided song)\n",
    "  - Put Song in, get similar artist (you could also put multiple songs in, but I dont think that this is worth it)\n",
    "  - Simulate entering a whole user profile, from which we can take the average song data and get new artists this way (which are not in here)\n",
    "- Things missing\n",
    "  - We dont have the release date or listening date, so we cannot use time as a feature. This could create even better recommendations, because we would know what the user currently listens to and weigh it  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all Files in Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_and_concatenate_parquet_files(folder_path):\n",
    "    # List all files in the directory\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith('.parquet')]\n",
    "    \n",
    "    # Sort files for consistent order if needed (optional)\n",
    "    files.sort()\n",
    "\n",
    "    # Load and concatenate all the Parquet files\n",
    "    df_list = []\n",
    "    for file in files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_parquet(file_path)\n",
    "        df_list.append(df)\n",
    "\n",
    "    # Concatenate all dataframes\n",
    "    concatenated_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    return concatenated_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save File in Format for GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "def save_dataframe_as_parquet(df:pd.DataFrame, folder_path=\"data\", folder_name=None, always_overwrite=None, model_object=None):\n",
    "    if not folder_name:\n",
    "        folder_name = input(\"Enter the name of the folder to save the files: \")\n",
    "\n",
    "    full_path = os.path.join(folder_path, folder_name)\n",
    "\n",
    "    # Check if the folder already exists\n",
    "    if os.path.exists(full_path) and always_overwrite is not True:\n",
    "        if always_overwrite is None:\n",
    "            overwrite = input(f\"The folder '{folder_name}' already exists. Do you want to overwrite it? (yes/no): \")\n",
    "            always_overwrite = overwrite.lower() != 'yes'\n",
    "        if not always_overwrite:\n",
    "            suffix = 1\n",
    "            new_folder_name = f\"{folder_name}_{suffix}\"\n",
    "            while os.path.exists(os.path.join(folder_path, new_folder_name)):\n",
    "                suffix += 1\n",
    "                new_folder_name = f\"{folder_name}_{suffix}\"\n",
    "            folder_name = new_folder_name\n",
    "            full_path = os.path.join(folder_path, folder_name)\n",
    "    \n",
    "    os.makedirs(full_path, exist_ok=True)\n",
    "\n",
    "    temp_file = os.path.join(full_path, \"temp.parquet\")\n",
    "    df.to_parquet(temp_file)\n",
    "    file_size = os.path.getsize(temp_file) / (1024 * 1024)  # Size in MB\n",
    "    os.remove(temp_file)\n",
    "\n",
    "    if file_size > 50:\n",
    "        num_splits = math.ceil(file_size / 50)\n",
    "        row_split = math.ceil(len(df) / num_splits)\n",
    "    else:\n",
    "        num_splits = 1\n",
    "        row_split = len(df)\n",
    "    \n",
    "    for i in range(num_splits):\n",
    "        start_row = i * row_split\n",
    "        end_row = min((i + 1) * row_split, len(df))\n",
    "        split_df = df.iloc[start_row:end_row]\n",
    "        split_file_name = os.path.join(full_path, f\"{folder_name}_part_{i + 1}.parquet\")\n",
    "        split_df.to_parquet(split_file_name)\n",
    "    \n",
    "    if model_object:\n",
    "        for key, value in model_object.items():\n",
    "            joblib.dump(value, f'{full_path}/{key}_model.pkl')\n",
    "\n",
    "    print(f\"Dataframe saved in {num_splits} files under the folder: {full_path}\")\n",
    "    \n",
    "    return full_path\n",
    "\n",
    "# Example usage:\n",
    "# save_dataframe_as_parquet(df=kmeans_cluster, folder_path=\"data\", folder_name=\"kmeans_clustered_subset\", always_overwrite=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and View Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_name</th>\n",
       "      <th>track_name</th>\n",
       "      <th>track_id</th>\n",
       "      <th>popularity</th>\n",
       "      <th>year</th>\n",
       "      <th>genre</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jason Mraz</td>\n",
       "      <td>I Won't Give Up</td>\n",
       "      <td>53QF56cjZA9RTuuMZDrSA6</td>\n",
       "      <td>68.0</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>acoustic</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.303</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-10.058</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0429</td>\n",
       "      <td>0.6940</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1150</td>\n",
       "      <td>0.139</td>\n",
       "      <td>133.406</td>\n",
       "      <td>240166.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jason Mraz</td>\n",
       "      <td>93 Million Miles</td>\n",
       "      <td>1s8tP3jP4GZcyHDsjvw218</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>acoustic</td>\n",
       "      <td>0.572</td>\n",
       "      <td>0.454</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>0.4770</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.515</td>\n",
       "      <td>140.182</td>\n",
       "      <td>216387.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Joshua Hyslop</td>\n",
       "      <td>Do Not Let Me Go</td>\n",
       "      <td>7BRCa8MPiyuvr2VU3O9W0F</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>acoustic</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.234</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-13.711</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.3380</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.145</td>\n",
       "      <td>139.832</td>\n",
       "      <td>158960.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boyce Avenue</td>\n",
       "      <td>Fast Car</td>\n",
       "      <td>63wsZUhUZLlh1OsyrZq7sz</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>acoustic</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.251</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-9.845</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.8070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0797</td>\n",
       "      <td>0.508</td>\n",
       "      <td>204.961</td>\n",
       "      <td>304293.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Andrew Belle</td>\n",
       "      <td>Sky's Still Blue</td>\n",
       "      <td>6nXIYClvJAfi6ujLiKqEq8</td>\n",
       "      <td>54.0</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>acoustic</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.791</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-5.419</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0302</td>\n",
       "      <td>0.0726</td>\n",
       "      <td>0.019300</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.217</td>\n",
       "      <td>171.864</td>\n",
       "      <td>244320.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     artist_name        track_name                track_id  popularity  \\\n",
       "0     Jason Mraz   I Won't Give Up  53QF56cjZA9RTuuMZDrSA6        68.0   \n",
       "1     Jason Mraz  93 Million Miles  1s8tP3jP4GZcyHDsjvw218        50.0   \n",
       "2  Joshua Hyslop  Do Not Let Me Go  7BRCa8MPiyuvr2VU3O9W0F        57.0   \n",
       "3   Boyce Avenue          Fast Car  63wsZUhUZLlh1OsyrZq7sz        58.0   \n",
       "4   Andrew Belle  Sky's Still Blue  6nXIYClvJAfi6ujLiKqEq8        54.0   \n",
       "\n",
       "        year     genre  danceability  energy   key  loudness  mode  \\\n",
       "0 2012-01-01  acoustic         0.483   0.303   4.0   -10.058   1.0   \n",
       "1 2012-01-01  acoustic         0.572   0.454   3.0   -10.286   1.0   \n",
       "2 2012-01-01  acoustic         0.409   0.234   3.0   -13.711   1.0   \n",
       "3 2012-01-01  acoustic         0.392   0.251  10.0    -9.845   1.0   \n",
       "4 2012-01-01  acoustic         0.430   0.791   6.0    -5.419   0.0   \n",
       "\n",
       "   speechiness  acousticness  instrumentalness  liveness  valence    tempo  \\\n",
       "0       0.0429        0.6940          0.000000    0.1150    0.139  133.406   \n",
       "1       0.0258        0.4770          0.000014    0.0974    0.515  140.182   \n",
       "2       0.0323        0.3380          0.000050    0.0895    0.145  139.832   \n",
       "3       0.0363        0.8070          0.000000    0.0797    0.508  204.961   \n",
       "4       0.0302        0.0726          0.019300    0.1100    0.217  171.864   \n",
       "\n",
       "   duration_ms  time_signature  \n",
       "0     240166.0             3.0  \n",
       "1     216387.0             4.0  \n",
       "2     158960.0             4.0  \n",
       "3     304293.0             4.0  \n",
       "4     244320.0             4.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>popularity</th>\n",
       "      <th>year</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.013313e+06</td>\n",
       "      <td>2013313</td>\n",
       "      <td>2.013313e+06</td>\n",
       "      <td>2.013313e+06</td>\n",
       "      <td>2.013313e+06</td>\n",
       "      <td>2.013313e+06</td>\n",
       "      <td>2.013313e+06</td>\n",
       "      <td>2.013313e+06</td>\n",
       "      <td>2.013313e+06</td>\n",
       "      <td>2.013313e+06</td>\n",
       "      <td>2.013313e+06</td>\n",
       "      <td>2.013313e+06</td>\n",
       "      <td>2.013313e+06</td>\n",
       "      <td>2.013313e+06</td>\n",
       "      <td>2.013313e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.848877e+01</td>\n",
       "      <td>2005-10-05 01:00:34.158026624</td>\n",
       "      <td>5.491744e-01</td>\n",
       "      <td>5.904303e-01</td>\n",
       "      <td>5.256363e+00</td>\n",
       "      <td>-9.799186e+00</td>\n",
       "      <td>6.422682e-01</td>\n",
       "      <td>1.010894e-01</td>\n",
       "      <td>3.714914e-01</td>\n",
       "      <td>2.211444e-01</td>\n",
       "      <td>2.162194e-01</td>\n",
       "      <td>4.809441e-01</td>\n",
       "      <td>1.200666e+02</td>\n",
       "      <td>2.385117e+05</td>\n",
       "      <td>3.878141e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1886-01-01 00:00:00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-6.000000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.073000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>2002-01-01 00:00:00</td>\n",
       "      <td>4.280000e-01</td>\n",
       "      <td>3.870000e-01</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>-1.217600e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.600000e-02</td>\n",
       "      <td>2.060000e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9.790000e-02</td>\n",
       "      <td>2.540000e-01</td>\n",
       "      <td>9.700300e+01</td>\n",
       "      <td>1.751250e+05</td>\n",
       "      <td>4.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.500000e+01</td>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>5.650000e-01</td>\n",
       "      <td>6.220000e-01</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>-8.334000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.870000e-02</td>\n",
       "      <td>2.490000e-01</td>\n",
       "      <td>4.380000e-04</td>\n",
       "      <td>1.330000e-01</td>\n",
       "      <td>4.740000e-01</td>\n",
       "      <td>1.200100e+02</td>\n",
       "      <td>2.185470e+05</td>\n",
       "      <td>4.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000e+01</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>6.860000e-01</td>\n",
       "      <td>8.250000e-01</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>-5.815000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>8.800000e-02</td>\n",
       "      <td>7.210000e-01</td>\n",
       "      <td>4.240000e-01</td>\n",
       "      <td>2.790000e-01</td>\n",
       "      <td>7.020000e-01</td>\n",
       "      <td>1.382540e+02</td>\n",
       "      <td>2.739870e+05</td>\n",
       "      <td>4.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>2023-01-01 00:00:00</td>\n",
       "      <td>9.990000e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>6.172000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.710000e-01</td>\n",
       "      <td>9.960000e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.499930e+02</td>\n",
       "      <td>6.000495e+06</td>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.674866e+01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.819063e-01</td>\n",
       "      <td>2.708000e-01</td>\n",
       "      <td>3.549231e+00</td>\n",
       "      <td>5.787686e+00</td>\n",
       "      <td>4.793327e-01</td>\n",
       "      <td>1.522333e-01</td>\n",
       "      <td>3.599918e-01</td>\n",
       "      <td>3.519784e-01</td>\n",
       "      <td>1.926347e-01</td>\n",
       "      <td>2.695143e-01</td>\n",
       "      <td>2.996391e+01</td>\n",
       "      <td>1.405136e+05</td>\n",
       "      <td>4.832763e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         popularity                           year  danceability  \\\n",
       "count  2.013313e+06                        2013313  2.013313e+06   \n",
       "mean   1.848877e+01  2005-10-05 01:00:34.158026624  5.491744e-01   \n",
       "min    0.000000e+00            1886-01-01 00:00:00  0.000000e+00   \n",
       "25%    3.000000e+00            2002-01-01 00:00:00  4.280000e-01   \n",
       "50%    1.500000e+01            2011-01-01 00:00:00  5.650000e-01   \n",
       "75%    3.000000e+01            2018-01-01 00:00:00  6.860000e-01   \n",
       "max    1.000000e+02            2023-01-01 00:00:00  9.990000e-01   \n",
       "std    1.674866e+01                            NaN  1.819063e-01   \n",
       "\n",
       "             energy           key      loudness          mode   speechiness  \\\n",
       "count  2.013313e+06  2.013313e+06  2.013313e+06  2.013313e+06  2.013313e+06   \n",
       "mean   5.904303e-01  5.256363e+00 -9.799186e+00  6.422682e-01  1.010894e-01   \n",
       "min    0.000000e+00  0.000000e+00 -6.000000e+01  0.000000e+00  0.000000e+00   \n",
       "25%    3.870000e-01  2.000000e+00 -1.217600e+01  0.000000e+00  3.600000e-02   \n",
       "50%    6.220000e-01  5.000000e+00 -8.334000e+00  1.000000e+00  4.870000e-02   \n",
       "75%    8.250000e-01  8.000000e+00 -5.815000e+00  1.000000e+00  8.800000e-02   \n",
       "max    1.000000e+00  1.100000e+01  6.172000e+00  1.000000e+00  9.710000e-01   \n",
       "std    2.708000e-01  3.549231e+00  5.787686e+00  4.793327e-01  1.522333e-01   \n",
       "\n",
       "       acousticness  instrumentalness      liveness       valence  \\\n",
       "count  2.013313e+06      2.013313e+06  2.013313e+06  2.013313e+06   \n",
       "mean   3.714914e-01      2.211444e-01  2.162194e-01  4.809441e-01   \n",
       "min    0.000000e+00      0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    2.060000e-02      0.000000e+00  9.790000e-02  2.540000e-01   \n",
       "50%    2.490000e-01      4.380000e-04  1.330000e-01  4.740000e-01   \n",
       "75%    7.210000e-01      4.240000e-01  2.790000e-01  7.020000e-01   \n",
       "max    9.960000e-01      1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "std    3.599918e-01      3.519784e-01  1.926347e-01  2.695143e-01   \n",
       "\n",
       "              tempo   duration_ms  time_signature  \n",
       "count  2.013313e+06  2.013313e+06    2.013313e+06  \n",
       "mean   1.200666e+02  2.385117e+05    3.878141e+00  \n",
       "min    0.000000e+00  2.073000e+03    0.000000e+00  \n",
       "25%    9.700300e+01  1.751250e+05    4.000000e+00  \n",
       "50%    1.200100e+02  2.185470e+05    4.000000e+00  \n",
       "75%    1.382540e+02  2.739870e+05    4.000000e+00  \n",
       "max    2.499930e+02  6.000495e+06    5.000000e+00  \n",
       "std    2.996391e+01  1.405136e+05    4.832763e-01  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2013313 entries, 0 to 2013312\n",
      "Data columns (total 19 columns):\n",
      " #   Column            Dtype         \n",
      "---  ------            -----         \n",
      " 0   artist_name       object        \n",
      " 1   track_name        object        \n",
      " 2   track_id          object        \n",
      " 3   popularity        float64       \n",
      " 4   year              datetime64[ns]\n",
      " 5   genre             object        \n",
      " 6   danceability      float64       \n",
      " 7   energy            float64       \n",
      " 8   key               float64       \n",
      " 9   loudness          float64       \n",
      " 10  mode              float64       \n",
      " 11  speechiness       float64       \n",
      " 12  acousticness      float64       \n",
      " 13  instrumentalness  float64       \n",
      " 14  liveness          float64       \n",
      " 15  valence           float64       \n",
      " 16  tempo             float64       \n",
      " 17  duration_ms       float64       \n",
      " 18  time_signature    float64       \n",
      "dtypes: datetime64[ns](1), float64(14), object(4)\n",
      "memory usage: 291.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "original_data = load_and_concatenate_parquet_files(\"data/spotify_data\")\n",
    "\n",
    "original_data[\"year\"] = pd.to_datetime(original_data[\"year\"], format='%Y')\n",
    "original_data = original_data.dropna(subset=[\"danceability\", \"energy\", \"key\", \"loudness\", \"mode\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\", \"duration_ms\", \"time_signature\", \"popularity\", \"track_id\", \"track_name\", \"artist_name\", \"year\"])\n",
    "original_data = original_data.drop_duplicates(subset=[\"track_name\", \"artist_name\", \"danceability\", \"energy\", \"key\", \"mode\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\", \"time_signature\"])\n",
    "original_data = original_data.reset_index(drop=True)\n",
    "original_data = original_data.drop(columns=[\"Unnamed: 0\"])\n",
    "display(original_data.head())\n",
    "display(original_data.describe())\n",
    "print(original_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Engine code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe saved in 1 files under the folder: data/kmeans_clustered_subset_1\n",
      "KMeans_cluster distro: kmeans_cluster\n",
      "2    5690\n",
      "1    4588\n",
      "0    3818\n",
      "3    3712\n",
      "4    2325\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_name</th>\n",
       "      <th>track_name</th>\n",
       "      <th>track_id</th>\n",
       "      <th>popularity</th>\n",
       "      <th>year</th>\n",
       "      <th>genre</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>kmeans_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1422250</th>\n",
       "      <td>Nikos Oikonomopoulos</td>\n",
       "      <td>Etsi Nomizis (Ke Mi Girisis)</td>\n",
       "      <td>2HSZYfpV0Ekn1SLoEM7Hv7</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.68000</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-5.776</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0458</td>\n",
       "      <td>0.38000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3980</td>\n",
       "      <td>0.4420</td>\n",
       "      <td>92.053</td>\n",
       "      <td>245560.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981539</th>\n",
       "      <td>Neil Diamond</td>\n",
       "      <td>Lost In Hollywood</td>\n",
       "      <td>01RlA9MxNRgmpY07diyu4p</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1986-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.55100</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-10.949</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.07590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0505</td>\n",
       "      <td>0.7470</td>\n",
       "      <td>127.864</td>\n",
       "      <td>261600.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713715</th>\n",
       "      <td>The Future Sound Of London</td>\n",
       "      <td>Papua New Guinea (Mellow Magic Maze Mix)</td>\n",
       "      <td>4adx3PhkJAog6RQPhUOhEL</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2002-01-01</td>\n",
       "      <td>breakbeat</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.88800</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-8.796</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0373</td>\n",
       "      <td>0.02410</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.3450</td>\n",
       "      <td>0.4840</td>\n",
       "      <td>124.985</td>\n",
       "      <td>330791.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992052</th>\n",
       "      <td>Paul Baloche</td>\n",
       "      <td>All Praise and Honor - Live</td>\n",
       "      <td>6Bxkrsitofy3w2mxxszYWH</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2008-01-01</td>\n",
       "      <td>french</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.48200</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-8.308</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.52400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3590</td>\n",
       "      <td>0.1880</td>\n",
       "      <td>142.795</td>\n",
       "      <td>443067.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305086</th>\n",
       "      <td>Dog Music</td>\n",
       "      <td>The Best Dog Music</td>\n",
       "      <td>13fECrGugy30hYjI2h9zim</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>sleep</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.00102</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-37.190</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0469</td>\n",
       "      <td>0.96500</td>\n",
       "      <td>0.737000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0929</td>\n",
       "      <td>79.356</td>\n",
       "      <td>118387.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431807</th>\n",
       "      <td>Eddie Griffin</td>\n",
       "      <td>\"We Need to Talk\"</td>\n",
       "      <td>6XMcxsYbrpAWUexgn3uTBw</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>comedy</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.66500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-11.775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9260</td>\n",
       "      <td>0.80500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.8380</td>\n",
       "      <td>0.6240</td>\n",
       "      <td>103.190</td>\n",
       "      <td>271018.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797597</th>\n",
       "      <td>Memphis Reigns</td>\n",
       "      <td>Red Vines</td>\n",
       "      <td>6eXb14CbsaxiSCBUEbZm6T</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.69800</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-9.564</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3680</td>\n",
       "      <td>0.36300</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.2930</td>\n",
       "      <td>0.2920</td>\n",
       "      <td>173.143</td>\n",
       "      <td>223256.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1701413</th>\n",
       "      <td>Danstar Lord</td>\n",
       "      <td>Go Away</td>\n",
       "      <td>2wDHQJ4tpZR158tCaPP7Ex</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.68700</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-5.589</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0507</td>\n",
       "      <td>0.02360</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0671</td>\n",
       "      <td>0.7190</td>\n",
       "      <td>139.982</td>\n",
       "      <td>179929.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222252</th>\n",
       "      <td>Black Sheep</td>\n",
       "      <td>The Choice Is Yours (Revisited)</td>\n",
       "      <td>4k9EkhkFZY8Bk41Qi0Ob7P</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1991-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.67900</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-12.028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2070</td>\n",
       "      <td>0.03360</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0742</td>\n",
       "      <td>0.6800</td>\n",
       "      <td>101.293</td>\n",
       "      <td>243200.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942700</th>\n",
       "      <td>Lifetime</td>\n",
       "      <td>Airport Monday Morning</td>\n",
       "      <td>72FKOjP70pWI8IvmveTzVl</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2007-01-01</td>\n",
       "      <td>emo</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.97900</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-3.320</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3110</td>\n",
       "      <td>0.00021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3660</td>\n",
       "      <td>0.4860</td>\n",
       "      <td>178.601</td>\n",
       "      <td>154240.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20133 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        artist_name                                track_name  \\\n",
       "1422250        Nikos Oikonomopoulos              Etsi Nomizis (Ke Mi Girisis)   \n",
       "1981539                Neil Diamond                         Lost In Hollywood   \n",
       "713715   The Future Sound Of London  Papua New Guinea (Mellow Magic Maze Mix)   \n",
       "992052                 Paul Baloche               All Praise and Honor - Live   \n",
       "305086                    Dog Music                        The Best Dog Music   \n",
       "...                             ...                                       ...   \n",
       "431807                Eddie Griffin                         \"We Need to Talk\"   \n",
       "1797597              Memphis Reigns                                 Red Vines   \n",
       "1701413                Danstar Lord                                   Go Away   \n",
       "1222252                 Black Sheep           The Choice Is Yours (Revisited)   \n",
       "942700                     Lifetime                    Airport Monday Morning   \n",
       "\n",
       "                       track_id  popularity       year      genre  \\\n",
       "1422250  2HSZYfpV0Ekn1SLoEM7Hv7        24.0 2009-01-01       None   \n",
       "1981539  01RlA9MxNRgmpY07diyu4p        23.0 1986-01-01       None   \n",
       "713715   4adx3PhkJAog6RQPhUOhEL         4.0 2002-01-01  breakbeat   \n",
       "992052   6Bxkrsitofy3w2mxxszYWH         8.0 2008-01-01     french   \n",
       "305086   13fECrGugy30hYjI2h9zim        21.0 2017-01-01      sleep   \n",
       "...                         ...         ...        ...        ...   \n",
       "431807   6XMcxsYbrpAWUexgn3uTBw        11.0 2020-01-01     comedy   \n",
       "1797597  6eXb14CbsaxiSCBUEbZm6T         0.0 2023-01-01       None   \n",
       "1701413  2wDHQJ4tpZR158tCaPP7Ex         0.0 2023-01-01       None   \n",
       "1222252  4k9EkhkFZY8Bk41Qi0Ob7P        59.0 1991-01-01       None   \n",
       "942700   72FKOjP70pWI8IvmveTzVl        17.0 2007-01-01        emo   \n",
       "\n",
       "         danceability   energy   key  loudness  mode  speechiness  \\\n",
       "1422250         0.502  0.68000  11.0    -5.776   0.0       0.0458   \n",
       "1981539         0.754  0.55100   7.0   -10.949   1.0       0.0294   \n",
       "713715          0.513  0.88800   3.0    -8.796   1.0       0.0373   \n",
       "992052          0.391  0.48200   9.0    -8.308   1.0       0.0316   \n",
       "305086          0.130  0.00102   3.0   -37.190   1.0       0.0469   \n",
       "...               ...      ...   ...       ...   ...          ...   \n",
       "431807          0.633  0.66500   1.0   -11.775   1.0       0.9260   \n",
       "1797597         0.426  0.69800   9.0    -9.564   1.0       0.3680   \n",
       "1701413         0.876  0.68700   4.0    -5.589   1.0       0.0507   \n",
       "1222252         0.867  0.67900  10.0   -12.028   0.0       0.2070   \n",
       "942700          0.185  0.97900  11.0    -3.320   1.0       0.3110   \n",
       "\n",
       "         acousticness  instrumentalness  liveness  valence    tempo  \\\n",
       "1422250       0.38000          0.000000    0.3980   0.4420   92.053   \n",
       "1981539       0.07590          0.000000    0.0505   0.7470  127.864   \n",
       "713715        0.02410          0.400000    0.3450   0.4840  124.985   \n",
       "992052        0.52400          0.000000    0.3590   0.1880  142.795   \n",
       "305086        0.96500          0.737000    0.1000   0.0929   79.356   \n",
       "...               ...               ...       ...      ...      ...   \n",
       "431807        0.80500          0.000000    0.8380   0.6240  103.190   \n",
       "1797597       0.36300          0.000002    0.2930   0.2920  173.143   \n",
       "1701413       0.02360          0.000000    0.0671   0.7190  139.982   \n",
       "1222252       0.03360          0.000002    0.0742   0.6800  101.293   \n",
       "942700        0.00021          0.000000    0.3660   0.4860  178.601   \n",
       "\n",
       "         duration_ms  time_signature  kmeans_cluster  \n",
       "1422250     245560.0             4.0               1  \n",
       "1981539     261600.0             4.0               2  \n",
       "713715      330791.0             4.0               3  \n",
       "992052      443067.0             4.0               3  \n",
       "305086      118387.0             5.0               4  \n",
       "...              ...             ...             ...  \n",
       "431807      271018.0             3.0               0  \n",
       "1797597     223256.0             4.0               3  \n",
       "1701413     179929.0             4.0               2  \n",
       "1222252     243200.0             4.0               1  \n",
       "942700      154240.0             4.0               3  \n",
       "\n",
       "[20133 rows x 20 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import hdbscan\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def reduce_data(data, dimensions):\n",
    "    numeric_columns = data[metric_columns]\n",
    "    pca_standardized = PCA(n_components=dimensions)\n",
    "    pca_standardized_result = pca_standardized.fit_transform(numeric_columns)\n",
    "    return pca_standardized_result\n",
    "\n",
    "def normalized_data(data:pd.DataFrame):\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    numeric_columns = data[metric_columns]\n",
    "    other_columns = data.drop(columns=metric_columns).reset_index(drop=True)\n",
    "    normalized_data = min_max_scaler.fit_transform(numeric_columns)\n",
    "    normalized_df = pd.DataFrame(normalized_data, columns=numeric_columns.columns)\n",
    "    normalized_data = pd.merge(normalized_df, other_columns, left_index=True, right_index=True)\n",
    "    return normalized_df\n",
    "\n",
    "def run_kmeans_and_hdbscan(original_data:pd.DataFrame, cluster_data:pd.DataFrame, kmeans_clusters=5, hdbscan_min_cluster_size=10):\n",
    "    data_for_clustering = cluster_data.copy()\n",
    "    kmeans = KMeans(n_clusters=kmeans_clusters, random_state=42)\n",
    "    kmeans_labels = kmeans.fit_predict(data_for_clustering)\n",
    "\n",
    "    data_for_clustering[\"kmeans_cluster\"] = kmeans_labels\n",
    "    original_data[\"kmeans_cluster\"] = kmeans_labels\n",
    "    print(f\"KMeans_cluster distro: {data_for_clustering['kmeans_cluster'].value_counts()}\")\n",
    "    hdbscan_cluster_df = []\n",
    "    hdbscan_models = {}\n",
    "    for cluster, data in data_for_clustering.groupby(\"kmeans_cluster\"):\n",
    "        hdbscan_clusterer = hdbscan.HDBSCAN(min_cluster_size=hdbscan_min_cluster_size)\n",
    "        hdbscan_cluster_labels = hdbscan_clusterer.fit_predict(data)\n",
    "        current_original_data = original_data[original_data[\"kmeans_cluster\"] == cluster].reset_index(drop=True).copy()\n",
    "        current_original_data[\"hdbscan_cluster\"] = hdbscan_cluster_labels\n",
    "        hdbscan_cluster_df.append(current_original_data)\n",
    "        hdbscan_models[cluster] = hdbscan_clusterer\n",
    "        print(f\"HDBSCAN_cluster distro for kmeans {cluster}: {current_original_data['hdbscan_cluster'].value_counts()}\")\n",
    "\n",
    "    all_cluster_results = pd.concat(hdbscan_cluster_df).reset_index(drop=True)\n",
    "    \n",
    "    return all_cluster_results, kmeans, hdbscan_models\n",
    "\n",
    "\n",
    "def cluster_data(original_data:pd.DataFrame, subset_fraction=None, kmeans=True, use_hdbscan=True, kmeans_clusters=5, hdbscan_min_cluster_size=10, standardize_data=True, pca_dimensions=None, columns_to_use=None):\n",
    "    \n",
    "    if subset_fraction:\n",
    "        original_data = original_data.sample(frac=subset_fraction)\n",
    "    \n",
    "    data_for_clustering = original_data.copy().reset_index(drop=True)\n",
    "    \n",
    "    if standardize_data:\n",
    "        data_for_clustering = normalized_data(data_for_clustering)\n",
    "\n",
    "    if pca_dimensions:\n",
    "        data_for_clustering = reduce_data(data_for_clustering, pca_dimensions)\n",
    "    \n",
    "    if pca_dimensions is None:\n",
    "        metric_columns = columns_to_use if columns_to_use else [\"danceability\", \"energy\", \"key\", \"loudness\", \"mode\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\", \"time_signature\"]\n",
    "        data_for_clustering = data_for_clustering[metric_columns]\n",
    "\n",
    "    if kmeans and use_hdbscan:\n",
    "        result_df, kmeans, hdbscan_models = run_kmeans_and_hdbscan(original_data, data_for_clustering, kmeans_clusters, hdbscan_min_cluster_size)\n",
    "        folder_path = save_dataframe_as_parquet(result_df, folder_path=\"data\", folder_name=\"kmeans_hdbscan_clustered_subset\", always_overwrite=False, model_object={\"kmeans\": kmeans})\n",
    "        for key, value in hdbscan_models.items():\n",
    "            joblib.dump(value, f'{folder_path}/hdbscan_model_{key}.pkl')\n",
    "        result = result_df.copy()\n",
    "    \n",
    "    elif kmeans:\n",
    "        kmeans = KMeans(n_clusters=kmeans_clusters, random_state=42)\n",
    "        kmeans_labels = kmeans.fit_predict(data_for_clustering)\n",
    "        original_data[\"kmeans_cluster\"] = kmeans_labels\n",
    "        save_dataframe_as_parquet(original_data, folder_path=\"data\", folder_name=\"kmeans_clustered_subset\", always_overwrite=False, model_object={\"kmeans\": kmeans})\n",
    "        print(f\"KMeans_cluster distro: {original_data['kmeans_cluster'].value_counts()}\")\n",
    "        result = original_data.copy()\n",
    "        \n",
    "    elif use_hdbscan:\n",
    "        hdbscan_clusterer = hdbscan.HDBSCAN(min_cluster_size=hdbscan_min_cluster_size)\n",
    "        hdbscan_labels = hdbscan_clusterer.fit_predict(data_for_clustering)\n",
    "        original_data[\"hdbscan_cluster\"] = hdbscan_labels\n",
    "        save_dataframe_as_parquet(original_data, folder_path=\"data\", folder_name=\"hdbscan_clustered_subset\", always_overwrite=False, model_object={\"hdbscan\": hdbscan_clusterer})\n",
    "        print(f\"HDBSCAN_cluster distro: {original_data['hdbscan_cluster'].value_counts()}\")\n",
    "        result = original_data.copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "clustered_data = cluster_data(original_data, subset_fraction=0.01, kmeans=True, use_hdbscan=False, kmeans_clusters=5, hdbscan_min_cluster_size=10, standardize_data=True, pca_dimensions=None, columns_to_use=None)\n",
    "display(clustered_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import process, utils\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import hdbscan\n",
    "import joblib\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyOAuth\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "class MusicRecommendation:\n",
    "    def __init__(self, original_data: pd.DataFrame, weight_year: float, weight_popularity: float):\n",
    "\n",
    "        load_dotenv()\n",
    "\n",
    "        # Initialize the Spotify client with authentication\n",
    "        auth_manager = SpotifyOAuth(client_id=os.getenv('SPOTIFY_CLIENT_ID'), client_secret=os.getenv('SPOTIFY_CLIENT_SECRET'), redirect_uri=os.getenv('SPOTIFY_REDIRECT_URI'), scope=\"user-library-read\")\n",
    "        self.sp = spotipy.Spotify(auth_manager=auth_manager)\n",
    "        self.original_data = original_data\n",
    "        self.weight_year = weight_year\n",
    "        self.weight_popularity = weight_popularity\n",
    "        self.clustering_method = \"kmeans\"\n",
    "        self.hdbscan_model = joblib.load(\"hdbscan_model.pkl\")\n",
    "        self.kmeans_model = joblib.load(\"kmeans_model.pkl\")\n",
    "        self.clustered_data = pd.read_parquet(\"clustered_subset.parquet\") if self.clustering_method == \"hdbscan\" else load_and_concatenate_parquet_files(\"data/kmean_clustered_subset\")\n",
    "        self.metrics = [\n",
    "            'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', \n",
    "            'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', \n",
    "            'time_signature'\n",
    "        ]\n",
    "    \n",
    "    def get_track_features(self, song_name: str, artist_name: str) -> pd.DataFrame:\n",
    "        results = self.sp.search(q=f\"track:{song_name} artist:{artist_name}\", type=\"track\", limit=1)\n",
    "        \n",
    "        if not results['tracks']['items']:\n",
    "            print(\"Track not found\")\n",
    "            return pd.DataFrame()  # Return an empty DataFrame if the track is not found\n",
    "        \n",
    "        track = results['tracks']['items'][0]\n",
    "        track_id = track['id']\n",
    "        track_name = track['name']\n",
    "        artist_name = track['artists'][0]['name']\n",
    "        popularity = track['popularity']\n",
    "        release_date = track['album']['release_date']\n",
    "        year = int(release_date.split('-')[0])\n",
    "        duration_ms = track['duration_ms']\n",
    "        audio_features = self.sp.audio_features(track_id)[0]\n",
    "        \n",
    "        data = {\n",
    "            'danceability': audio_features['danceability'],\n",
    "            'energy': audio_features['energy'],\n",
    "            'key': audio_features['key'],\n",
    "            'loudness': audio_features['loudness'],\n",
    "            'mode': audio_features['mode'],\n",
    "            'speechiness': audio_features['speechiness'],\n",
    "            'acousticness': audio_features['acousticness'],\n",
    "            'instrumentalness': audio_features['instrumentalness'],\n",
    "            'liveness': audio_features['liveness'],\n",
    "            'valence': audio_features['valence'],\n",
    "            'tempo': audio_features['tempo'],\n",
    "            'time_signature': audio_features['time_signature'],\n",
    "            'artist_name': artist_name,\n",
    "            'track_name': track_name,\n",
    "            'track_id': track_id,\n",
    "            'popularity': popularity,\n",
    "            'year': year,\n",
    "            'duration_ms': duration_ms\n",
    "        }\n",
    "        return pd.DataFrame([data])\n",
    "\n",
    "    def normalized_data(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        min_max_scaler = MinMaxScaler()\n",
    "        numeric_columns = data[self.metrics]\n",
    "        other_columns = data.drop(columns=self.metrics).reset_index(drop=True)\n",
    "        normalized_data = min_max_scaler.fit_transform(numeric_columns)\n",
    "        return pd.merge(pd.DataFrame(normalized_data, columns=self.metrics), other_columns, left_index=True, right_index=True)\n",
    "    \n",
    "    def reduce_data(self, data: pd.DataFrame, dimensions: int) -> np.ndarray:\n",
    "        numeric_columns = data[self.metrics]\n",
    "        pca = PCA(n_components=dimensions)\n",
    "        return pca.fit_transform(numeric_columns)\n",
    "\n",
    "    def get_closest_match(self, user_input: str, df: pd.DataFrame, column: str, threshold: int = 92) -> str:\n",
    "        processed_user_input = utils.default_process(user_input)\n",
    "        strings_column = df[column].dropna()\n",
    "        processed_strings = [utils.default_process(string) for string in strings_column]\n",
    "        match = process.extractOne(processed_user_input, processed_strings, processor=None, score_cutoff=threshold)\n",
    "        return strings_column.iloc[match[2]] if match else None\n",
    "\n",
    "    def song_finder(self, song_name: str, artist_name: str, data) -> pd.DataFrame:\n",
    "        song = data[(data[\"track_name\"] == song_name) & (data[\"artist_name\"] == artist_name)]\n",
    "        return song if not song.empty else None\n",
    "\n",
    "    def preprocess_song(self, song: pd.DataFrame, normalization: str, reduction: str) -> pd.DataFrame:\n",
    "        if normalization == \"standardized\":\n",
    "            song = self.standardized_data(song)\n",
    "        elif normalization == \"normalized\":\n",
    "            song = self.normalized_data(song)\n",
    "        if reduction == \"pca\":\n",
    "            song = self.reduce_data(song, 2)\n",
    "        return song\n",
    "\n",
    "    def get_song_cluster(self, song: pd.DataFrame) -> int:\n",
    "        if self.clustering_method == \"hdbscan\":\n",
    "            new_data_point = song[self.metrics].values.reshape(1, -1)\n",
    "            predicted_cluster, _ = hdbscan.approximate_predict(self.hdbscan_model, new_data_point)\n",
    "            cluster = predicted_cluster[0]\n",
    "            \n",
    "        elif self.clustering_method == \"kmeans\":\n",
    "            cluster = self.kmeans_model.predict(song[self.metrics])[0]\n",
    "        return cluster\n",
    "\n",
    "    def find_nearest_neighbors(self, song: pd.DataFrame, data: pd.DataFrame, number_of_songs: int) -> tuple:\n",
    "        knn_model = NearestNeighbors(n_neighbors=100)\n",
    "        cluster_data = data[self.metrics]\n",
    "        knn_model.fit(cluster_data)\n",
    "        distances, indices = knn_model.kneighbors(song[self.metrics], n_neighbors=100)\n",
    "        neighbors_df = data.iloc[indices[0]]\n",
    "        return neighbors_df, distances[0]\n",
    "\n",
    "    def get_weighted_scores(self, neighbors_df: pd.DataFrame, neighbor_distances: np.ndarray) -> pd.DataFrame:\n",
    "        # Ensure we are working with a copy of the DataFrame to avoid SettingWithCopyWarning\n",
    "        neighbors_df = neighbors_df.copy()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Convert 'year' column to numeric\n",
    "        neighbors_df['year'] = pd.to_datetime(neighbors_df['year'], format='%Y')\n",
    "        neighbors_df['year_numeric'] = neighbors_df['year'].dt.year\n",
    "        \n",
    "        # Normalize 'year_numeric' and 'popularity' columns\n",
    "        scaler = MinMaxScaler()\n",
    "        neighbors_df[['year_normalized', 'popularity_normalized']] = scaler.fit_transform(\n",
    "            neighbors_df[['year_numeric', 'popularity']]\n",
    "        )\n",
    "        \n",
    "        year_normalized = neighbors_df['year_normalized'].values\n",
    "        popularity_normalized = neighbors_df['popularity_normalized'].values\n",
    "        \n",
    "        # Define weights for year and popularity\n",
    "        year_weight = self.weight_year\n",
    "        popularity_weight = self.weight_popularity\n",
    "\n",
    "        # Compute base scores (inverse distance, to ensure higher similarity has a higher base score)\n",
    "        base_scores = 1 / (neighbor_distances + 1e-8)  # Avoid division by zero\n",
    "\n",
    "        # Compute boosting scores\n",
    "        boosting_scores = year_normalized * year_weight + popularity_normalized * popularity_weight\n",
    "\n",
    "        # Final scores: add boosting scores to base scores\n",
    "        final_scores = base_scores + boosting_scores\n",
    "\n",
    "        # Rank neighbors based on weighted scores\n",
    "        ranked_indices = np.argsort(final_scores)[::-1]  # Sort in descending order\n",
    "        return neighbors_df.iloc[ranked_indices]\n",
    "\n",
    "    def print_preview_urls(self, song_df: pd.DataFrame) -> None:\n",
    "        for _, row in song_df.iterrows():\n",
    "            track_id = row['track_id']\n",
    "            track = self.sp.track(track_id)\n",
    "            preview_url = track.get('preview_url')\n",
    "            if preview_url:\n",
    "                print(f\"Track: {row['track_name']} by {row['artist_name']}\")\n",
    "                print(f\"Preview URL: {preview_url}\")\n",
    "            else:\n",
    "                print(f\"Track: {row['track_name']} by {row['artist_name']}\")\n",
    "                print(\"Preview URL not available.\")\n",
    "    \n",
    "    def choose_correct_song(self, song: pd.DataFrame) -> pd.DataFrame:\n",
    "        if len(song) > 1:\n",
    "            chosen_index = int(input(\"Enter the index of the song you want to use: \"))\n",
    "            if chosen_index >= len(song):\n",
    "                print(\"Invalid index, using 1st song\")\n",
    "                chosen_index = 0\n",
    "            song = song.iloc[chosen_index].copy()\n",
    "        return song\n",
    "    \n",
    "    def remove_song_if_present(self, song: pd.DataFrame, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        matched_rows = data[(data[\"track_name\"] == song[\"track_name\"].values[0]) & (data[\"artist_name\"] == song[\"artist_name\"].values[0])]\n",
    "        if not matched_rows.empty:\n",
    "            print(f\"Removing {len(matched_rows)} rows from possible recommendations\")\n",
    "            if len(matched_rows) > 1:\n",
    "                print(\"Rows to be removed:\")\n",
    "                display(matched_rows.head(5))\n",
    "            indexes_to_remove = matched_rows.index\n",
    "            return data.drop(indexes_to_remove)\n",
    "        else:\n",
    "            return data\n",
    "    \n",
    "    \n",
    "    def find_closest_songs(self, song_name: str = \"\", artist_name: str = \"\", same_artist: bool = \"\", number_of_songs: int = \"\", weighted_system: bool = \"\") -> pd.DataFrame:\n",
    "        if artist_name == \"\":\n",
    "            artist_name = input(\"Enter the artist name: \")\n",
    "        if song_name == \"\":\n",
    "            song_name = input(\"Enter the song name: \")\n",
    "        if same_artist == \"\":\n",
    "            same_artist = input(\"Filter by same artist? (yes/no): \").strip().lower() == 'yes'\n",
    "        if number_of_songs == \"\":\n",
    "            number_of_songs = int(input(\"Enter the number of songs to return: \"))\n",
    "        if weighted_system == \"\":\n",
    "            weighted_system = input(\"Use weighted system? (yes/no): \").strip().lower() == 'yes'\n",
    "\n",
    "        song = self.get_track_features(song_name, artist_name)\n",
    "        \n",
    "        if song is None:\n",
    "            print(\"No match found\")\n",
    "            return None\n",
    "        \n",
    "        print(\"Song found\")\n",
    "        real_artist_name = song[\"artist_name\"].values[0]\n",
    "        real_song_name = song[\"track_name\"].values[0]\n",
    "        print(f\"Using {real_song_name} by {real_artist_name}\")\n",
    "        display(song.reset_index(drop=True))\n",
    "        self.print_preview_urls(song)\n",
    "        \n",
    "        song = self.choose_correct_song(song)\n",
    "        \n",
    "        clustered_data = self.remove_song_if_present(song, self.clustered_data)\n",
    "        \n",
    "        all_songs = pd.concat([clustered_data, song], ignore_index=True) \n",
    "        \n",
    "        all_song_standardized = self.normalized_data(all_songs)\n",
    "        song_standardized = self.song_finder(real_song_name, real_artist_name, all_song_standardized)\n",
    "        \n",
    "        predicted_cluster = self.get_song_cluster(song_standardized)\n",
    "        \n",
    "        print(f\"\\nPredicted cluster: {predicted_cluster}\\n\")\n",
    "        \n",
    "        sample_data = all_song_standardized[all_song_standardized[\"cluster\"] == predicted_cluster]\n",
    "        if not same_artist:\n",
    "            sample_data = sample_data[sample_data[\"artist_name\"] != artist_name]\n",
    "         \n",
    "        neighbors_df, neighbor_distances = self.find_nearest_neighbors(song_standardized, sample_data, number_of_songs)\n",
    "        \n",
    "        if weighted_system:\n",
    "            neighbors_df = self.get_weighted_scores(neighbors_df, neighbor_distances)\n",
    "        \n",
    "        closest_songs = self.original_data.loc[neighbors_df.head(number_of_songs).index]\n",
    "        \n",
    "        closest_songs = closest_songs[song.columns]\n",
    "        \n",
    "        self.print_preview_urls(closest_songs)\n",
    "        \n",
    "        return closest_songs\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize with default paths and parameters\n",
    "   \n",
    "    recommender = MusicRecommendation(original_data = original_data, weight_year=0.6, weight_popularity=0.4)\n",
    "\n",
    "    # Find closest songs with\n",
    "\n",
    "recommender.find_closest_songs(artist_name=\"Skrillex\", song_name=\"bangarang\", same_artist=True, number_of_songs=5, weighted_system=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here the modelling and transformation starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Select the numeric columns\n",
    "numeric_columns = feature_df.drop(columns=[\"track_id\", \"genre\"])\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# Standardize the numeric columns\n",
    "standardized_data = standard_scaler.fit_transform(numeric_columns)\n",
    "standardized_df = pd.DataFrame(standardized_data, columns=numeric_columns.columns)\n",
    "standardized_df['genre'] = feature_df['genre']\n",
    "standardized_df['track_id'] = feature_df['track_id']\n",
    "\n",
    "# Normalize the numeric columns\n",
    "normalized_data = min_max_scaler.fit_transform(numeric_columns)\n",
    "normalized_df = pd.DataFrame(normalized_data, columns=numeric_columns.columns)\n",
    "normalized_df['genre'] = feature_df['genre']\n",
    "normalized_df['track_id'] = feature_df['track_id']\n",
    "\n",
    "# Display the standardized and normalized dataframes\n",
    "display(standardized_df.describe())\n",
    "display(normalized_df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(standardized_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "# Perform PCA on standardized_data\n",
    "pca_standardized = PCA(n_components=2)\n",
    "pca_standardized_result = pca_standardized.fit_transform(standardized_data)\n",
    "print(1)\n",
    "\n",
    "# Perform PCA on normalized_data\n",
    "pca_normalized = PCA(n_components=2)\n",
    "pca_normalized_result = pca_normalized.fit_transform(normalized_data)\n",
    "print(2)\n",
    "\n",
    "# # Perform t-SNE on standardized_data\n",
    "# tsne_standardized = TSNE(n_components=2)\n",
    "# tsne_standardized_result = tsne_standardized.fit_transform(standardized_data)\n",
    "# print(3)\n",
    "\n",
    "# # Perform t-SNE on normalized_data\n",
    "# tsne_normalized = TSNE(n_components=2)\n",
    "# tsne_normalized_result = tsne_normalized.fit_transform(normalized_data)\n",
    "# print(4)\n",
    "\n",
    "# # Create the subplot with 4 plots\n",
    "# fig = px.subplots(\n",
    "#     rows=2, cols=2,\n",
    "#     subplot_titles=(\"PCA - Standardized Data\", \"PCA - Normalized Data\", \"t-SNE - Standardized Data\", \"t-SNE - Normalized Data\"),\n",
    "#     shared_xaxes=True, shared_yaxes=True,\n",
    "#     vertical_spacing=0.1, horizontal_spacing=0.1\n",
    "# )\n",
    "\n",
    "# # Add PCA - Standardized Data plot\n",
    "# fig.add_trace(\n",
    "#     px.scatter(x=pca_standardized_result[:, 0], y=pca_standardized_result[:, 1], color=standardized_df['track_genre']).data[0],\n",
    "#     row=1, col=1\n",
    "# )\n",
    "\n",
    "# # Add PCA - Normalized Data plot\n",
    "# fig.add_trace(\n",
    "#     px.scatter(x=pca_normalized_result[:, 0], y=pca_normalized_result[:, 1], color=normalized_df['track_genre']).data[0],\n",
    "#     row=1, col=2\n",
    "# )\n",
    "\n",
    "# # Add t-SNE - Standardized Data plot\n",
    "# fig.add_trace(\n",
    "#     px.scatter(x=tsne_standardized_result[:, 0], y=tsne_standardized_result[:, 1], color=standardized_df['track_genre']).data[0],\n",
    "#     row=2, col=1\n",
    "# )\n",
    "\n",
    "# # Add t-SNE - Normalized Data plot\n",
    "# fig.add_trace(\n",
    "#     px.scatter(x=tsne_normalized_result[:, 0], y=tsne_normalized_result[:, 1], color=normalized_df['track_genre']).data[0],\n",
    "#     row=2, col=2\n",
    "# )\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(\n",
    "#     height=800,\n",
    "#     showlegend=False\n",
    "# )\n",
    "\n",
    "# # Show the subplot\n",
    "# fig.show()\n",
    "\n",
    "px.scatter(x=pca_standardized_result[:, 0], y=pca_standardized_result[:, 1], color=standardized_df['genre']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(original_data, hue='track_genre', diag_kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# import plotly.express as px\n",
    "\n",
    "# dataframe = standardized_df.copy()\n",
    "# # Assuming 'data' is your dataframe and track_genre is a column in the dataframe\n",
    "\n",
    "# # Create a subset of the data\n",
    "# subset_data = dataframe.sample(n=1000, random_state=42)\n",
    "\n",
    "# # Prepare the data: Separate features and labels\n",
    "# features = subset_data.drop(columns=['track_genre', \"track_id\"])  # Drop the track_genre column\n",
    "# labels = subset_data['track_genre']  # Save the track_genre column separately\n",
    "\n",
    "# # Apply t-SNE\n",
    "# tsne = TSNE(n_components=2, random_state=42)\n",
    "# tsne_results = tsne.fit_transform(features)\n",
    "\n",
    "# # Create a DataFrame for the t-SNE results\n",
    "# tsne_df = pd.DataFrame(tsne_results, columns=['tsne_1', 'tsne_2'])\n",
    "# tsne_df['track_genre'] = labels.values\n",
    "\n",
    "# # Plot the results using Plotly Express\n",
    "# fig = px.scatter(tsne_df, x='tsne_1', y='tsne_2', color='track_genre', title='t-SNE of Track Features by Genre')\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import umap\n",
    "\n",
    "# reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "\n",
    "# # Apply UMAP\n",
    "# umap_results = reducer.fit_transform(subset_data.drop(columns=['track_genre', \"track_id\"]))\n",
    "\n",
    "# px.scatter(x=umap_results[:, 0], y=umap_results[:, 1], color=subset_data['track_genre']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from datetime import datetime\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # a function to get content-based recommendations based on music features\n",
    "# def content_based_recommendations(input_song_name, num_recommendations=5):\n",
    "#     if input_song_name not in music_df['Track Name'].values:\n",
    "#         print(f\"'{input_song_name}' not found in the dataset. Please enter a valid song name.\")\n",
    "#         return\n",
    "\n",
    "#     # Get the index of the input song in the music DataFrame\n",
    "#     input_song_index = music_df[music_df['Track Name'] == input_song_name].index[0]\n",
    "\n",
    "#     # Calculate the similarity scores based on music features (cosine similarity)\n",
    "#     similarity_scores = cosine_similarity([music_features_scaled[input_song_index]], music_features_scaled)\n",
    "\n",
    "#     # Get the indices of the most similar songs\n",
    "#     similar_song_indices = similarity_scores.argsort()[0][::-1][1:num_recommendations + 1]\n",
    "\n",
    "#     # Get the names of the most similar songs based on content-based filtering\n",
    "#     content_based_recommendations = music_df.iloc[similar_song_indices][['Track Name', 'Artists', 'Album Name', 'Release Date', 'Popularity']]\n",
    "\n",
    "#     return content_based_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Loop through a range of cluster numbers to calculate silhouette scores\n",
    "silhouette_scores = []\n",
    "cluster_range = range(2, 26)\n",
    "data_sample = standardized_data[np.random.choice(standardized_data.shape[0], 20000, replace=False)]\n",
    "for k in cluster_range:\n",
    "    print(f\"Calculating silhouette score for k = {k}\")\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(data_sample)\n",
    "    silhouette_avg = silhouette_score(data_sample, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "    print(f\"For n_clusters = {k}, the average silhouette score is {silhouette_avg:.4f}\")\n",
    "\n",
    "# Optionally, you can plot the silhouette scores\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(cluster_range, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Scores for k-means clustering')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "data_sample = standardized_data[np.random.choice(standardized_data.shape[0], 200000, replace=False)]\n",
    "\n",
    "# Fit the HDBSCAN model\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=100)\n",
    "hdbscan_model.fit(data_sample)\n",
    "\n",
    "# Get the labels assigned to each data point\n",
    "cluster_labels = hdbscan_model.labels_\n",
    "\n",
    "# Example: Print out the first 10 cluster labels\n",
    "print(\"First 10 cluster labels:\", cluster_labels[:10])\n",
    "\n",
    "# Print out the number of clusters found (excluding noise)\n",
    "print(f\"Number of clusters found: {len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def song_finder(song_name, artist_name):\n",
    "    song = clustered_data[(clustered_data[\"track_name\"] == song_name) & (clustered_data[\"artist_name\"] == artist_name)]\n",
    "    return song\n",
    "\n",
    "song = song_finder(\"Shape of You\", \"Ed Sheeran\")\n",
    "\n",
    "standardized_data[song.index]\n",
    "\n",
    "for song in clustered_data[['track_name', 'artist_name']].itertuples():\n",
    "    print(song[0])\n",
    "    print(standardized_data[song[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def song_finder(song_name, artist_name):\n",
    "    song = clustered_data[(clustered_data[\"track_name\"] == song_name) & (clustered_data[\"artist_name\"] == artist_name)]\n",
    "    return song\n",
    "\n",
    "def find_closest_songs(song_name, artist_name, song_number=5):\n",
    "    all_distances = []\n",
    "    \n",
    "    chosen_song = song_finder(song_name, artist_name)\n",
    "    index = chosen_song.index\n",
    "    print(index)\n",
    "    print(standardized_data[index][0])\n",
    "    for song in clustered_data[['track_name', 'artist_name']].itertuples():\n",
    "\n",
    "        current_distance = distance.cosine(standardized_data[song[0]],standardized_data[chosen_song.index][0])\n",
    "        all_distances.append((song.track_name, song.artist_name, current_distance))\n",
    "    all_distances.sort(key=lambda x: x[2], reverse=False)\n",
    "    return all_distances[1:song_number+1]\n",
    "\n",
    "find_closest_songs(\"Shape of You\", \"Skrillex\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

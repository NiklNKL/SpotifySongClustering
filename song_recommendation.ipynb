{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "- https://www.kaggle.com/datasets/notshrirang/spotify-million-song-dataset\n",
    "- https://www.kaggle.com/datasets/tonygordonjr/spotify-dataset-2023?select=spotify-albums_data_2023.csv\n",
    "- https://www.kaggle.com/datasets/yamaerenay/spotify-dataset-19212020-600k-tracks?select=tracks.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links\n",
    "- https://forecastegy.com/posts/xgboost-multiclass-classification-python/\n",
    "- https://github.com/jannine92/spotify_recommendation/blob/main/music_recommender.ipynb\n",
    "- https://www.kaggle.com/code/nyjoey/spotify-clustering\n",
    "- https://ausaf-a.github.io/ml-song-recommender/\n",
    "- https://medium.com/@Marlon_H/spotify-clustering-f41b40003c9a\n",
    "- https://www.kaggle.com/code/choongqianzheng/song-genre-classification-system\n",
    "- https://developer.spotify.com/documentation/web-api/reference/get-audio-features\n",
    "- https://medium.com/@miguelrodrigueznovelo/discover-your-perfect-playlist-10-songs-recommended-by-a-music-recommendation-system-with-python-5fd246d87127\n",
    "- https://medium.com/@shruti.somankar/building-a-music-recommendation-system-using-spotify-api-and-python-f7418a21fa41\n",
    "- https://www.kaggle.com/code/merveeyuboglu/music-recommendation-system-cosine-s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo List:\n",
    "\n",
    "- Basic stuff✅\n",
    "  - Load Data✅\n",
    "  -  Display, Info and Describe data✅\n",
    "  - Split Datasets into song_metrics and song_info✅\n",
    "- Data Visualization (Also in Percent if valuable)✅\n",
    "  - Visualize Correlation Heatmap✅\n",
    "  - Display Genres as Numbers and Histogram✅\n",
    "  - Display Genre Dendogram✅\n",
    "  - Display most frequent artists✅\n",
    "  - Display most popular artist✅\n",
    "  - Plot Popularity as histogram✅\n",
    "  - Plot Average Song metric for Each genre (could also be on a 3D plot)✅\n",
    "  - Plot Box plots to detect outliers✅\n",
    "- Features\n",
    "  - Apply Standard and MinMaxScaler ✅\n",
    "  - Apply and Visualize PCA and t-SNE / UMAP\n",
    "  - Use Silhouette Score to see how many clusters are needed (also try fancy plot from Medium)\n",
    "  - Use KMeans to start\n",
    "  - Use DBSCAN\n",
    "  - Use Agglomerative Clustering\n",
    "  - Use HDBSCAN\n",
    "  - Use XGBClassifier with Cross Validation\n",
    "- Feature Extensions\n",
    "  - Plot Similar Artists\n",
    "  - Plot Similar Genres\n",
    "  - (Plot Similar Songs [Only small set of Data here])\n",
    "- Possible Uses:\n",
    "  - Put song into spotify api, get song data back, and use that to find similar songs (with possibility to get different artists than the one from the provided song)\n",
    "  - Put Song in, get similar artist (you could also put multiple songs in, but I dont think that this is worth it)\n",
    "  - Simulate entering a whole user profile, from which we can take the average song data and get new artists this way (which are not in here)\n",
    "- Things missing\n",
    "  - We dont have the release date or listening date, so we cannot use time as a feature. This could create even better recommendations, because we would know what the user currently listens to and weigh it  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and View Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = []\n",
    "for i in range(3):\n",
    "    data = pd.read_parquet(f'spotify_data_part_{i+1}.parquet')\n",
    "    results.append(data)\n",
    "\n",
    "original_data = pd.concat(results)\n",
    "\n",
    "\n",
    "original_data[\"year\"] = pd.to_datetime(original_data[\"year\"], format='%Y')\n",
    "original_data = original_data.dropna(subset=[\"danceability\", \"energy\", \"key\", \"loudness\", \"mode\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\", \"duration_ms\", \"time_signature\", \"popularity\", \"track_id\", \"track_name\", \"artist_name\", \"year\"])\n",
    "original_data = original_data.drop_duplicates(subset=[\"track_name\", \"artist_name\", \"danceability\", \"energy\", \"key\", \"mode\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\", \"time_signature\"])\n",
    "original_data = original_data.reset_index(drop=True)\n",
    "original_data = original_data.drop(columns=[\"Unnamed: 0\"])\n",
    "display(original_data.head())\n",
    "display(original_data.describe())\n",
    "print(original_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Engine code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "metric_columns = [\"danceability\", \"energy\", \"key\", \"loudness\", \"mode\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\", \"time_signature\"]\n",
    "\n",
    "def standardized_data(data:pd.DataFrame):\n",
    "    standard_scaler = StandardScaler()\n",
    "    numeric_columns = data[metric_columns]\n",
    "    other_columns = data.drop(columns=metric_columns).reset_index(drop=True)\n",
    "    standardized_data = standard_scaler.fit_transform(numeric_columns)\n",
    "    standardized_df = pd.DataFrame(standardized_data, columns=numeric_columns.columns)\n",
    "    standardized_df = pd.merge(standardized_df, other_columns, left_index=True, right_index=True, how=\"left\")\n",
    "    return standardized_df\n",
    "\n",
    "def normalized_data(data:pd.DataFrame):\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    numeric_columns = data[metric_columns]\n",
    "    other_columns = data.drop(columns=metric_columns).reset_index(drop=True)\n",
    "    normalized_data = min_max_scaler.fit_transform(numeric_columns)\n",
    "    normalized_df = pd.DataFrame(normalized_data, columns=numeric_columns.columns)\n",
    "    normalized_data = pd.merge(normalized_df, other_columns, left_index=True, right_index=True)\n",
    "    return normalized_df\n",
    "\n",
    "def reduce_data(data, dimensions):\n",
    "    numeric_columns = data[metric_columns]\n",
    "    pca_standardized = PCA(n_components=dimensions)\n",
    "    pca_standardized_result = pca_standardized.fit_transform(numeric_columns)\n",
    "    return pca_standardized_result\n",
    "\n",
    "\n",
    "import hdbscan\n",
    "import joblib\n",
    "\n",
    "original_data_subset = original_data.sample(frac=0.02)\n",
    "\n",
    "original_data_subset = standardized_data(original_data_subset)\n",
    "display(original_data_subset)\n",
    "# original_data_subset = normalized_data(original_data_subset)\n",
    "# original_data_subset = reduce_data(original_data_subset, 2)\n",
    "\n",
    "data_for_clustering = original_data_subset[metric_columns]\n",
    "\n",
    "hdbscan_clusterer = hdbscan.HDBSCAN(min_cluster_size=10, prediction_data=True)\n",
    "hdbscan_clusterer.fit(data_for_clustering)\n",
    "joblib.dump(hdbscan_clusterer, 'hdbscan_model.pkl')\n",
    "\n",
    "clustered_subset = original_data_subset.copy()\n",
    "clustered_subset[\"cluster\"] = hdbscan_clusterer.labels_\n",
    "clustered_subset.to_parquet(\"clustered_subset.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import process, utils\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import hdbscan\n",
    "import joblib\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyOAuth\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "class MusicRecommendation:\n",
    "    def __init__(self):\n",
    "\n",
    "        load_dotenv()\n",
    "\n",
    "        # Initialize the Spotify client with authentication\n",
    "        auth_manager = SpotifyOAuth(client_id=os.getenv('SPOTIFY_CLIENT_ID'), client_secret=os.getenv('SPOTIFY_CLIENT_SECRET'), redirect_uri=os.getenv('SPOTIFY_REDIRECT_URI'), scope=\"user-library-read\")\n",
    "        self.sp = spotipy.Spotify(auth_manager=auth_manager)\n",
    "        self.model = joblib.load(\"hdbscan_model.pkl\")\n",
    "\n",
    "        self.original_data = pd.read_parquet(\"clustered_subset.parquet\")\n",
    "        self.metrics = [\n",
    "            'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', \n",
    "            'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', \n",
    "            'time_signature'\n",
    "        ]\n",
    "    \n",
    "    def get_track_features(self, song_name: str, artist_name: str) -> pd.DataFrame:\n",
    "        results = self.sp.search(q=f\"track:{song_name} artist:{artist_name}\", type=\"track\", limit=1)\n",
    "        \n",
    "        if not results['tracks']['items']:\n",
    "            print(\"Track not found\")\n",
    "            return pd.DataFrame()  # Return an empty DataFrame if the track is not found\n",
    "        \n",
    "        track = results['tracks']['items'][0]\n",
    "        track_id = track['id']\n",
    "        track_name = track['name']\n",
    "        artist_name = track['artists'][0]['name']\n",
    "        popularity = track['popularity']\n",
    "        release_date = track['album']['release_date']\n",
    "        year = int(release_date.split('-')[0])\n",
    "        duration_ms = track['duration_ms']\n",
    "        audio_features = self.sp.audio_features(track_id)[0]\n",
    "        \n",
    "        data = {\n",
    "            'danceability': audio_features['danceability'],\n",
    "            'energy': audio_features['energy'],\n",
    "            'key': audio_features['key'],\n",
    "            'loudness': audio_features['loudness'],\n",
    "            'mode': audio_features['mode'],\n",
    "            'speechiness': audio_features['speechiness'],\n",
    "            'acousticness': audio_features['acousticness'],\n",
    "            'instrumentalness': audio_features['instrumentalness'],\n",
    "            'liveness': audio_features['liveness'],\n",
    "            'valence': audio_features['valence'],\n",
    "            'tempo': audio_features['tempo'],\n",
    "            'time_signature': audio_features['time_signature'],\n",
    "            'artist_name': artist_name,\n",
    "            'track_name': track_name,\n",
    "            'track_id': track_id,\n",
    "            'popularity': popularity,\n",
    "            'year': year,\n",
    "            'duration_ms': duration_ms\n",
    "        }\n",
    "        return pd.DataFrame([data])\n",
    "    \n",
    "    def standardized_data(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        standard_scaler = StandardScaler()\n",
    "        numeric_columns = data[self.metrics]\n",
    "        other_columns = data.drop(columns=self.metrics).reset_index(drop=True)\n",
    "        standardized_data = standard_scaler.fit_transform(numeric_columns)\n",
    "        standardized_df = pd.DataFrame(standardized_data, columns=self.metrics)\n",
    "        return pd.merge(standardized_df, other_columns, left_index=True, right_index=True, how=\"left\")\n",
    "\n",
    "    def normalized_data(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        min_max_scaler = MinMaxScaler()\n",
    "        numeric_columns = data[self.metrics]\n",
    "        other_columns = data.drop(columns=self.metrics).reset_index(drop=True)\n",
    "        normalized_data = min_max_scaler.fit_transform(numeric_columns)\n",
    "        return pd.merge(pd.DataFrame(normalized_data, columns=self.metrics), other_columns, left_index=True, right_index=True)\n",
    "    \n",
    "    def reduce_data(self, data: pd.DataFrame, dimensions: int) -> np.ndarray:\n",
    "        numeric_columns = data[self.metrics]\n",
    "        pca = PCA(n_components=dimensions)\n",
    "        return pca.fit_transform(numeric_columns)\n",
    "\n",
    "    def get_closest_match(self, user_input: str, df: pd.DataFrame, column: str, threshold: int = 90) -> str:\n",
    "        processed_user_input = utils.default_process(user_input)\n",
    "        strings_column = df[column].dropna()\n",
    "        processed_strings = [utils.default_process(string) for string in strings_column]\n",
    "        match = process.extractOne(processed_user_input, processed_strings, processor=None, score_cutoff=threshold)\n",
    "        return strings_column.iloc[match[2]] if match else None\n",
    "\n",
    "    def song_finder(self, song_name: str, artist_name: str) -> pd.DataFrame:\n",
    "        song = self.original_data[(self.original_data[\"track_name\"] == song_name) & (self.original_data[\"artist_name\"] == artist_name)]\n",
    "        return song if not song.empty else None\n",
    "\n",
    "    def preprocess_song(self, song: pd.DataFrame, normalization: str, reduction: str) -> pd.DataFrame:\n",
    "        if normalization == \"standardized\":\n",
    "            song = self.standardized_data(song)\n",
    "        elif normalization == \"normalized\":\n",
    "            song = self.normalized_data(song)\n",
    "        if reduction == \"pca\":\n",
    "            song = self.reduce_data(song, 2)\n",
    "        return song\n",
    "\n",
    "    def get_song_cluster(self, song: pd.DataFrame) -> int:\n",
    "        new_data_point = song[self.metrics].values.reshape(1, -1)\n",
    "        model = self.model\n",
    "        predicted_cluster, _ = hdbscan.approximate_predict(model, new_data_point)\n",
    "        return predicted_cluster[0]\n",
    "\n",
    "    def find_nearest_neighbors(self, song: pd.DataFrame, data: pd.DataFrame, number_of_songs: int) -> tuple:\n",
    "        knn_model = NearestNeighbors(n_neighbors=100)\n",
    "        cluster_data = data[self.metrics]\n",
    "        knn_model.fit(cluster_data)\n",
    "        distances, indices = knn_model.kneighbors(song[self.metrics], n_neighbors=100)\n",
    "        neighbors_df = data.iloc[indices[0]]\n",
    "        return neighbors_df, distances[0]\n",
    "\n",
    "    def get_weighted_scores(self, neighbors_df: pd.DataFrame, neighbor_distances: np.ndarray) -> pd.DataFrame:\n",
    "        # Ensure we are working with a copy of the DataFrame to avoid SettingWithCopyWarning\n",
    "        neighbors_df = neighbors_df.copy()\n",
    "        \n",
    "        # Convert 'year' column to numeric\n",
    "        neighbors_df['year_numeric'] = neighbors_df['year'].dt.year\n",
    "        \n",
    "        # Normalize 'year_numeric' and 'popularity' columns\n",
    "        scaler = MinMaxScaler()\n",
    "        neighbors_df[['year_normalized', 'popularity_normalized']] = scaler.fit_transform(\n",
    "            neighbors_df[['year_numeric', 'popularity']]\n",
    "        )\n",
    "        \n",
    "        year_normalized = neighbors_df['year_normalized'].values\n",
    "        popularity_normalized = neighbors_df['popularity_normalized'].values\n",
    "        \n",
    "        # Define weights for year and popularity\n",
    "        year_weight = 0.4\n",
    "        popularity_weight = 0.6\n",
    "\n",
    "        # Compute base scores (inverse distance, to ensure higher similarity has a higher base score)\n",
    "        base_scores = 1 / (neighbor_distances + 1e-8)  # Avoid division by zero\n",
    "\n",
    "        # Compute boosting scores\n",
    "        boosting_scores = year_normalized * year_weight + popularity_normalized * popularity_weight\n",
    "\n",
    "        # Final scores: add boosting scores to base scores\n",
    "        final_scores = base_scores + boosting_scores\n",
    "\n",
    "        # Rank neighbors based on weighted scores\n",
    "        ranked_indices = np.argsort(final_scores)[::-1]  # Sort in descending order\n",
    "        return neighbors_df.iloc[ranked_indices]\n",
    "\n",
    "    def print_preview_urls(self, song_df: pd.DataFrame) -> None:\n",
    "        for _, row in song_df.iterrows():\n",
    "            track_id = row['track_id']\n",
    "            track = self.sp.track(track_id)\n",
    "            preview_url = track.get('preview_url')\n",
    "            if preview_url:\n",
    "                print(f\"Track: {row['track_name']} by {row['artist_name']}\")\n",
    "                print(f\"Preview URL: {preview_url}\")\n",
    "            else:\n",
    "                print(f\"Track: {row['track_name']} by {row['artist_name']}\")\n",
    "                print(\"Preview URL not available.\")\n",
    "\n",
    "    def find_closest_songs(self, song_name: str = \"\", artist_name: str = \"\", same_artist: bool = \"\", number_of_songs: int = \"\") -> pd.DataFrame:\n",
    "        if artist_name == \"\":\n",
    "            artist_name = input(\"Enter the artist name: \")\n",
    "        if song_name == \"\":\n",
    "            song_name = input(\"Enter the song name: \")\n",
    "        if same_artist == \"\":\n",
    "            same_artist = input(\"Filter by same artist? (yes/no): \").strip().lower() == 'yes'\n",
    "        if number_of_songs == \"\":\n",
    "            number_of_songs = int(input(\"Enter the number of songs to return: \"))\n",
    "\n",
    "        artist_name_corrected = self.get_closest_match(artist_name, self.original_data, \"artist_name\")\n",
    "        song_name_corrected = self.get_closest_match(song_name, self.original_data, \"track_name\")\n",
    "        \n",
    "        song = self.song_finder(song_name_corrected, artist_name_corrected)\n",
    "        if song is None:\n",
    "            song = self.get_track_features(song_name, artist_name)\n",
    "        \n",
    "        if song is None:\n",
    "            print(\"No match found\")\n",
    "            return None\n",
    "        \n",
    "        song_standardized = self.standardized_data(song)\n",
    "        predicted_cluster = self.get_song_cluster(song_standardized)\n",
    "        \n",
    "        sample_data = self.original_data[self.original_data[\"cluster\"] == predicted_cluster]\n",
    "        if not same_artist:\n",
    "            sample_data = sample_data[sample_data[\"artist_name\"] != artist_name]\n",
    "        \n",
    "        neighbors_df, neighbor_distances = self.find_nearest_neighbors(song, sample_data, number_of_songs)\n",
    "        neighbors_df = self.get_weighted_scores(neighbors_df, neighbor_distances)\n",
    "        \n",
    "        closest_songs = neighbors_df.head(number_of_songs)\n",
    "        \n",
    "        self.print_preview_urls(closest_songs)\n",
    "        \n",
    "        return closest_songs\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize with default paths and parameters\n",
    "   \n",
    "    recommender = MusicRecommendation()\n",
    "\n",
    "    # Find closest songs with\n",
    "\n",
    "recommender.find_closest_songs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here the modelling and transformation starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Select the numeric columns\n",
    "numeric_columns = feature_df.drop(columns=[\"track_id\", \"genre\"])\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# Standardize the numeric columns\n",
    "standardized_data = standard_scaler.fit_transform(numeric_columns)\n",
    "standardized_df = pd.DataFrame(standardized_data, columns=numeric_columns.columns)\n",
    "standardized_df['genre'] = feature_df['genre']\n",
    "standardized_df['track_id'] = feature_df['track_id']\n",
    "\n",
    "# Normalize the numeric columns\n",
    "normalized_data = min_max_scaler.fit_transform(numeric_columns)\n",
    "normalized_df = pd.DataFrame(normalized_data, columns=numeric_columns.columns)\n",
    "normalized_df['genre'] = feature_df['genre']\n",
    "normalized_df['track_id'] = feature_df['track_id']\n",
    "\n",
    "# Display the standardized and normalized dataframes\n",
    "display(standardized_df.describe())\n",
    "display(normalized_df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(standardized_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "# Perform PCA on standardized_data\n",
    "pca_standardized = PCA(n_components=2)\n",
    "pca_standardized_result = pca_standardized.fit_transform(standardized_data)\n",
    "print(1)\n",
    "\n",
    "# Perform PCA on normalized_data\n",
    "pca_normalized = PCA(n_components=2)\n",
    "pca_normalized_result = pca_normalized.fit_transform(normalized_data)\n",
    "print(2)\n",
    "\n",
    "# # Perform t-SNE on standardized_data\n",
    "# tsne_standardized = TSNE(n_components=2)\n",
    "# tsne_standardized_result = tsne_standardized.fit_transform(standardized_data)\n",
    "# print(3)\n",
    "\n",
    "# # Perform t-SNE on normalized_data\n",
    "# tsne_normalized = TSNE(n_components=2)\n",
    "# tsne_normalized_result = tsne_normalized.fit_transform(normalized_data)\n",
    "# print(4)\n",
    "\n",
    "# # Create the subplot with 4 plots\n",
    "# fig = px.subplots(\n",
    "#     rows=2, cols=2,\n",
    "#     subplot_titles=(\"PCA - Standardized Data\", \"PCA - Normalized Data\", \"t-SNE - Standardized Data\", \"t-SNE - Normalized Data\"),\n",
    "#     shared_xaxes=True, shared_yaxes=True,\n",
    "#     vertical_spacing=0.1, horizontal_spacing=0.1\n",
    "# )\n",
    "\n",
    "# # Add PCA - Standardized Data plot\n",
    "# fig.add_trace(\n",
    "#     px.scatter(x=pca_standardized_result[:, 0], y=pca_standardized_result[:, 1], color=standardized_df['track_genre']).data[0],\n",
    "#     row=1, col=1\n",
    "# )\n",
    "\n",
    "# # Add PCA - Normalized Data plot\n",
    "# fig.add_trace(\n",
    "#     px.scatter(x=pca_normalized_result[:, 0], y=pca_normalized_result[:, 1], color=normalized_df['track_genre']).data[0],\n",
    "#     row=1, col=2\n",
    "# )\n",
    "\n",
    "# # Add t-SNE - Standardized Data plot\n",
    "# fig.add_trace(\n",
    "#     px.scatter(x=tsne_standardized_result[:, 0], y=tsne_standardized_result[:, 1], color=standardized_df['track_genre']).data[0],\n",
    "#     row=2, col=1\n",
    "# )\n",
    "\n",
    "# # Add t-SNE - Normalized Data plot\n",
    "# fig.add_trace(\n",
    "#     px.scatter(x=tsne_normalized_result[:, 0], y=tsne_normalized_result[:, 1], color=normalized_df['track_genre']).data[0],\n",
    "#     row=2, col=2\n",
    "# )\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(\n",
    "#     height=800,\n",
    "#     showlegend=False\n",
    "# )\n",
    "\n",
    "# # Show the subplot\n",
    "# fig.show()\n",
    "\n",
    "px.scatter(x=pca_standardized_result[:, 0], y=pca_standardized_result[:, 1], color=standardized_df['genre']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(original_data, hue='track_genre', diag_kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# import plotly.express as px\n",
    "\n",
    "# dataframe = standardized_df.copy()\n",
    "# # Assuming 'data' is your dataframe and track_genre is a column in the dataframe\n",
    "\n",
    "# # Create a subset of the data\n",
    "# subset_data = dataframe.sample(n=1000, random_state=42)\n",
    "\n",
    "# # Prepare the data: Separate features and labels\n",
    "# features = subset_data.drop(columns=['track_genre', \"track_id\"])  # Drop the track_genre column\n",
    "# labels = subset_data['track_genre']  # Save the track_genre column separately\n",
    "\n",
    "# # Apply t-SNE\n",
    "# tsne = TSNE(n_components=2, random_state=42)\n",
    "# tsne_results = tsne.fit_transform(features)\n",
    "\n",
    "# # Create a DataFrame for the t-SNE results\n",
    "# tsne_df = pd.DataFrame(tsne_results, columns=['tsne_1', 'tsne_2'])\n",
    "# tsne_df['track_genre'] = labels.values\n",
    "\n",
    "# # Plot the results using Plotly Express\n",
    "# fig = px.scatter(tsne_df, x='tsne_1', y='tsne_2', color='track_genre', title='t-SNE of Track Features by Genre')\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import umap\n",
    "\n",
    "# reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "\n",
    "# # Apply UMAP\n",
    "# umap_results = reducer.fit_transform(subset_data.drop(columns=['track_genre', \"track_id\"]))\n",
    "\n",
    "# px.scatter(x=umap_results[:, 0], y=umap_results[:, 1], color=subset_data['track_genre']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from datetime import datetime\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # a function to get content-based recommendations based on music features\n",
    "# def content_based_recommendations(input_song_name, num_recommendations=5):\n",
    "#     if input_song_name not in music_df['Track Name'].values:\n",
    "#         print(f\"'{input_song_name}' not found in the dataset. Please enter a valid song name.\")\n",
    "#         return\n",
    "\n",
    "#     # Get the index of the input song in the music DataFrame\n",
    "#     input_song_index = music_df[music_df['Track Name'] == input_song_name].index[0]\n",
    "\n",
    "#     # Calculate the similarity scores based on music features (cosine similarity)\n",
    "#     similarity_scores = cosine_similarity([music_features_scaled[input_song_index]], music_features_scaled)\n",
    "\n",
    "#     # Get the indices of the most similar songs\n",
    "#     similar_song_indices = similarity_scores.argsort()[0][::-1][1:num_recommendations + 1]\n",
    "\n",
    "#     # Get the names of the most similar songs based on content-based filtering\n",
    "#     content_based_recommendations = music_df.iloc[similar_song_indices][['Track Name', 'Artists', 'Album Name', 'Release Date', 'Popularity']]\n",
    "\n",
    "#     return content_based_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Loop through a range of cluster numbers to calculate silhouette scores\n",
    "silhouette_scores = []\n",
    "cluster_range = range(2, 26)\n",
    "data_sample = standardized_data[np.random.choice(standardized_data.shape[0], 20000, replace=False)]\n",
    "for k in cluster_range:\n",
    "    print(f\"Calculating silhouette score for k = {k}\")\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(data_sample)\n",
    "    silhouette_avg = silhouette_score(data_sample, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "    print(f\"For n_clusters = {k}, the average silhouette score is {silhouette_avg:.4f}\")\n",
    "\n",
    "# Optionally, you can plot the silhouette scores\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(cluster_range, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Scores for k-means clustering')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "data_sample = standardized_data[np.random.choice(standardized_data.shape[0], 200000, replace=False)]\n",
    "\n",
    "# Fit the HDBSCAN model\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=100)\n",
    "hdbscan_model.fit(data_sample)\n",
    "\n",
    "# Get the labels assigned to each data point\n",
    "cluster_labels = hdbscan_model.labels_\n",
    "\n",
    "# Example: Print out the first 10 cluster labels\n",
    "print(\"First 10 cluster labels:\", cluster_labels[:10])\n",
    "\n",
    "# Print out the number of clusters found (excluding noise)\n",
    "print(f\"Number of clusters found: {len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def song_finder(song_name, artist_name):\n",
    "    song = original_data[(original_data[\"track_name\"] == song_name) & (original_data[\"artist_name\"] == artist_name)]\n",
    "    return song\n",
    "\n",
    "song = song_finder(\"Shape of You\", \"Ed Sheeran\")\n",
    "\n",
    "standardized_data[song.index]\n",
    "\n",
    "for song in original_data[['track_name', 'artist_name']].itertuples():\n",
    "    print(song[0])\n",
    "    print(standardized_data[song[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def song_finder(song_name, artist_name):\n",
    "    song = original_data[(original_data[\"track_name\"] == song_name) & (original_data[\"artist_name\"] == artist_name)]\n",
    "    return song\n",
    "\n",
    "def find_closest_songs(song_name, artist_name, song_number=5):\n",
    "    all_distances = []\n",
    "    \n",
    "    chosen_song = song_finder(song_name, artist_name)\n",
    "    index = chosen_song.index\n",
    "    print(index)\n",
    "    print(standardized_data[index][0])\n",
    "    for song in original_data[['track_name', 'artist_name']].itertuples():\n",
    "\n",
    "        current_distance = distance.cosine(standardized_data[song[0]],standardized_data[chosen_song.index][0])\n",
    "        all_distances.append((song.track_name, song.artist_name, current_distance))\n",
    "    all_distances.sort(key=lambda x: x[2], reverse=False)\n",
    "    return all_distances[1:song_number+1]\n",
    "\n",
    "find_closest_songs(\"Shape of You\", \"Skrillex\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

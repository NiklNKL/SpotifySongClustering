{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "- https://www.kaggle.com/datasets/notshrirang/spotify-million-song-dataset\n",
    "- https://www.kaggle.com/datasets/tonygordonjr/spotify-dataset-2023?select=spotify-albums_data_2023.csv\n",
    "- https://www.kaggle.com/datasets/yamaerenay/spotify-dataset-19212020-600k-tracks?select=tracks.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links\n",
    "- https://forecastegy.com/posts/xgboost-multiclass-classification-python/\n",
    "- https://github.com/jannine92/spotify_recommendation/blob/main/music_recommender.ipynb\n",
    "- https://www.kaggle.com/code/nyjoey/spotify-clustering\n",
    "- https://ausaf-a.github.io/ml-song-recommender/\n",
    "- https://medium.com/@Marlon_H/spotify-clustering-f41b40003c9a\n",
    "- https://www.kaggle.com/code/choongqianzheng/song-genre-classification-system\n",
    "- https://developer.spotify.com/documentation/web-api/reference/get-audio-features\n",
    "- https://medium.com/@miguelrodrigueznovelo/discover-your-perfect-playlist-10-songs-recommended-by-a-music-recommendation-system-with-python-5fd246d87127\n",
    "- https://medium.com/@shruti.somankar/building-a-music-recommendation-system-using-spotify-api-and-python-f7418a21fa41\n",
    "- https://www.kaggle.com/code/merveeyuboglu/music-recommendation-system-cosine-s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo List:\n",
    "\n",
    "- Basic stuff✅\n",
    "  - Load Data✅\n",
    "  -  Display, Info and Describe data✅\n",
    "  - Split Datasets into song_metrics and song_info✅\n",
    "- Data Visualization (Also in Percent if valuable)✅\n",
    "  - Visualize Correlation Heatmap✅\n",
    "  - Display Genres as Numbers and Histogram✅\n",
    "  - Display Genre Dendogram✅\n",
    "  - Display most frequent artists✅\n",
    "  - Display most popular artist✅\n",
    "  - Plot Popularity as histogram✅\n",
    "  - Plot Average Song metric for Each genre (could also be on a 3D plot)✅\n",
    "  - Plot Box plots to detect outliers✅\n",
    "- Features\n",
    "  - Apply Standard and MinMaxScaler ✅\n",
    "  - Apply and Visualize PCA and t-SNE / UMAP\n",
    "  - Use Silhouette Score to see how many clusters are needed (also try fancy plot from Medium)\n",
    "  - Use KMeans to start\n",
    "  - Use DBSCAN\n",
    "  - Use Agglomerative Clustering\n",
    "  - Use HDBSCAN\n",
    "  - Use XGBClassifier with Cross Validation\n",
    "- Feature Extensions\n",
    "  - Plot Similar Artists\n",
    "  - Plot Similar Genres\n",
    "  - (Plot Similar Songs [Only small set of Data here])\n",
    "- Possible Uses:\n",
    "  - Put song into spotify api, get song data back, and use that to find similar songs (with possibility to get different artists than the one from the provided song)\n",
    "  - Put Song in, get similar artist (you could also put multiple songs in, but I dont think that this is worth it)\n",
    "  - Simulate entering a whole user profile, from which we can take the average song data and get new artists this way (which are not in here)\n",
    "- Things missing\n",
    "  - We dont have the release date or listening date, so we cannot use time as a feature. This could create even better recommendations, because we would know what the user currently listens to and weigh it  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all Files in Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_and_concatenate_parquet_files(folder_path):\n",
    "    # List all files in the directory\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith('.parquet')]\n",
    "    \n",
    "    # Sort files for consistent order if needed (optional)\n",
    "    files.sort()\n",
    "\n",
    "    # Load and concatenate all the Parquet files\n",
    "    df_list = []\n",
    "    for file in files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_parquet(file_path)\n",
    "        df_list.append(df)\n",
    "\n",
    "    # Concatenate all dataframes\n",
    "    concatenated_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    return concatenated_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save File in Format for GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "def save_dataframe_as_parquet(df:pd.DataFrame, folder_path=\"data\", folder_name=None, always_overwrite=None, model_object=None):\n",
    "    if not folder_name:\n",
    "        folder_name = input(\"Enter the name of the folder to save the files: \")\n",
    "\n",
    "    full_path = os.path.join(folder_path, folder_name)\n",
    "\n",
    "    # Check if the folder already exists\n",
    "    if os.path.exists(full_path) and always_overwrite is not True:\n",
    "        if always_overwrite is None:\n",
    "            overwrite = input(f\"The folder '{folder_name}' already exists. Do you want to overwrite it? (yes/no): \")\n",
    "            always_overwrite = overwrite.lower() != 'yes'\n",
    "        if not always_overwrite:\n",
    "            suffix = 1\n",
    "            new_folder_name = f\"{folder_name}_{suffix}\"\n",
    "            while os.path.exists(os.path.join(folder_path, new_folder_name)):\n",
    "                suffix += 1\n",
    "                new_folder_name = f\"{folder_name}_{suffix}\"\n",
    "            folder_name = new_folder_name\n",
    "            full_path = os.path.join(folder_path, folder_name)\n",
    "    \n",
    "    os.makedirs(full_path, exist_ok=True)\n",
    "\n",
    "    temp_file = os.path.join(full_path, \"temp.parquet\")\n",
    "    df.to_parquet(temp_file)\n",
    "    file_size = os.path.getsize(temp_file) / (1024 * 1024)  # Size in MB\n",
    "    os.remove(temp_file)\n",
    "\n",
    "    if file_size > 50:\n",
    "        num_splits = math.ceil(file_size / 50)\n",
    "        row_split = math.ceil(len(df) / num_splits)\n",
    "    else:\n",
    "        num_splits = 1\n",
    "        row_split = len(df)\n",
    "    \n",
    "    for i in range(num_splits):\n",
    "        start_row = i * row_split\n",
    "        end_row = min((i + 1) * row_split, len(df))\n",
    "        split_df = df.iloc[start_row:end_row]\n",
    "        split_file_name = os.path.join(full_path, f\"{folder_name}_part_{i + 1}.parquet\")\n",
    "        split_df.to_parquet(split_file_name)\n",
    "    \n",
    "    if model_object:\n",
    "        for key, value in model_object.items():\n",
    "            joblib.dump(value, f'{full_path}/{key}_model.pkl')\n",
    "\n",
    "    print(f\"Dataframe saved in {num_splits} files under the folder: {full_path}\")\n",
    "    \n",
    "    return full_path\n",
    "\n",
    "# Example usage:\n",
    "# save_dataframe_as_parquet(df=kmeans_cluster, folder_path=\"data\", folder_name=\"kmeans_clustered_subset\", always_overwrite=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and View Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "original_data = load_and_concatenate_parquet_files(\"data/spotify_data\")\n",
    "\n",
    "original_data[\"year\"] = pd.to_datetime(original_data[\"year\"], format='%Y')\n",
    "original_data = original_data.dropna(subset=[\"danceability\", \"energy\", \"key\", \"loudness\", \"mode\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\", \"duration_ms\", \"time_signature\", \"popularity\", \"track_id\", \"track_name\", \"artist_name\", \"year\"])\n",
    "original_data = original_data.drop_duplicates(subset=[\"track_name\", \"artist_name\", \"danceability\", \"energy\", \"key\", \"mode\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\", \"time_signature\"])\n",
    "original_data = original_data.reset_index(drop=True)\n",
    "original_data = original_data.drop(columns=[\"Unnamed: 0\"])\n",
    "display(original_data.head())\n",
    "display(original_data.describe())\n",
    "print(original_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Engine code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import hdbscan\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def reduce_data(data, dimensions):\n",
    "    metric_columns = [\"danceability\", \"energy\", \"key\", \"loudness\", \"mode\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\", \"time_signature\"]\n",
    "    numeric_columns = data[metric_columns]\n",
    "    pca_standardized = PCA(n_components=dimensions)\n",
    "    pca_standardized_result = pca_standardized.fit_transform(numeric_columns)\n",
    "    return pca_standardized_result\n",
    "\n",
    "def normalized_data(data:pd.DataFrame, metric_columns):\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    numeric_columns = data[metric_columns]\n",
    "    other_columns = data.drop(columns=metric_columns).reset_index(drop=True)\n",
    "    normalized_data = min_max_scaler.fit_transform(numeric_columns)\n",
    "    normalized_df = pd.DataFrame(normalized_data, columns=numeric_columns.columns)\n",
    "    normalized_data = pd.merge(normalized_df, other_columns, left_index=True, right_index=True)\n",
    "    return normalized_df\n",
    "\n",
    "def run_kmeans_and_hdbscan(original_data:pd.DataFrame, cluster_data:pd.DataFrame, kmeans_clusters=5, hdbscan_min_cluster_size=10):\n",
    "    data_for_clustering = cluster_data.copy()\n",
    "    kmeans = KMeans(n_clusters=kmeans_clusters, random_state=42)\n",
    "    kmeans_labels = kmeans.fit_predict(data_for_clustering)\n",
    "\n",
    "    data_for_clustering[\"kmeans_cluster\"] = kmeans_labels\n",
    "    original_data[\"kmeans_cluster\"] = kmeans_labels\n",
    "    print(f\"KMeans_cluster distro: {data_for_clustering['kmeans_cluster'].value_counts()}\")\n",
    "    hdbscan_cluster_df = []\n",
    "    hdbscan_models = {}\n",
    "    for cluster, data in data_for_clustering.groupby(\"kmeans_cluster\"):\n",
    "        hdbscan_clusterer = hdbscan.HDBSCAN(min_cluster_size=hdbscan_min_cluster_size, min_samples=2)\n",
    "        hdbscan_cluster_labels = hdbscan_clusterer.fit_predict(data)\n",
    "        current_original_data = original_data[original_data[\"kmeans_cluster\"] == cluster].reset_index(drop=True).copy()\n",
    "        current_original_data[\"hdbscan_cluster\"] = hdbscan_cluster_labels\n",
    "        hdbscan_cluster_df.append(current_original_data)\n",
    "        hdbscan_models[cluster] = hdbscan_clusterer\n",
    "        print(f\"HDBSCAN_cluster distro for kmeans {cluster}: {current_original_data['hdbscan_cluster'].value_counts()}\")\n",
    "\n",
    "    all_cluster_results = pd.concat(hdbscan_cluster_df).reset_index(drop=True)\n",
    "    \n",
    "    return all_cluster_results, kmeans, hdbscan_models\n",
    "\n",
    "\n",
    "def cluster_data(original_data:pd.DataFrame, subset_fraction=None, kmeans=True, use_hdbscan=True, kmeans_clusters=5, hdbscan_min_cluster_size=10, standardize_data=True, pca_dimensions=None, columns_to_use=None):\n",
    "    \n",
    "    if subset_fraction:\n",
    "        original_data = original_data.sample(frac=subset_fraction)\n",
    "    \n",
    "    data_for_clustering = original_data.copy().reset_index(drop=True)\n",
    "    \n",
    "    metric_columns = columns_to_use if columns_to_use else [\"danceability\", \"energy\", \"key\", \"loudness\", \"mode\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\", \"time_signature\"]\n",
    "    \n",
    "    if standardize_data:\n",
    "        data_for_clustering = normalized_data(data_for_clustering, metric_columns=metric_columns)\n",
    "\n",
    "    if pca_dimensions:\n",
    "        data_for_clustering = reduce_data(data_for_clustering, pca_dimensions)\n",
    "    \n",
    "    if pca_dimensions is None:\n",
    "        data_for_clustering = data_for_clustering[metric_columns]\n",
    "\n",
    "    if kmeans and use_hdbscan:\n",
    "        result_df, kmeans, hdbscan_models = run_kmeans_and_hdbscan(original_data, data_for_clustering, kmeans_clusters, hdbscan_min_cluster_size)\n",
    "        folder_path = save_dataframe_as_parquet(result_df, folder_path=\"data\", folder_name=\"kmeans_hdbscan_clustered_subset\", always_overwrite=False, model_object={\"kmeans\": kmeans})\n",
    "        for key, value in hdbscan_models.items():\n",
    "            joblib.dump(value, f'{folder_path}/hdbscan_model_{key}.pkl')\n",
    "        result = result_df.copy()\n",
    "    \n",
    "    elif kmeans:\n",
    "        kmeans = KMeans(n_clusters=kmeans_clusters, random_state=42)\n",
    "        kmeans_labels = kmeans.fit_predict(data_for_clustering)\n",
    "        original_data[\"kmeans_cluster\"] = kmeans_labels\n",
    "        save_dataframe_as_parquet(original_data, folder_path=\"data\", folder_name=\"kmeans_clustered_subset\", always_overwrite=False, model_object={\"kmeans\": kmeans})\n",
    "        print(f\"KMeans_cluster distro: {original_data['kmeans_cluster'].value_counts()}\")\n",
    "        result = original_data.copy()\n",
    "        \n",
    "    elif use_hdbscan:\n",
    "        hdbscan_clusterer = hdbscan.HDBSCAN(min_cluster_size=hdbscan_min_cluster_size, min_samples=2)\n",
    "        hdbscan_labels = hdbscan_clusterer.fit_predict(data_for_clustering)\n",
    "        original_data[\"hdbscan_cluster\"] = hdbscan_labels\n",
    "        save_dataframe_as_parquet(original_data, folder_path=\"data\", folder_name=\"hdbscan_clustered_subset\", always_overwrite=False, model_object={\"hdbscan\": hdbscan_clusterer})\n",
    "        print(f\"HDBSCAN_cluster distro: {original_data['hdbscan_cluster'].value_counts()}\")\n",
    "        result = original_data.copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "clustered_data = cluster_data(original_data, subset_fraction=None, kmeans=True, use_hdbscan=True, kmeans_clusters=8, hdbscan_min_cluster_size=15, standardize_data=True, pca_dimensions=None, columns_to_use=None)\n",
    "display(clustered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px \n",
    "\n",
    "def reduce_data(data, dimensions):\n",
    "    metric_columns = [\"danceability\", \"energy\", \"key\", \"loudness\", \"mode\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\", \"time_signature\"]\n",
    "    numeric_columns = data[metric_columns]\n",
    "    pca_standardized = PCA(n_components=dimensions)\n",
    "    pca_standardized_result = pca_standardized.fit_transform(numeric_columns)\n",
    "    return pca_standardized_result\n",
    "\n",
    "new_df = clustered_data.copy()\n",
    "\n",
    "new_df[\"combined_cluster\"] = new_df[\"kmeans_cluster\"].astype(str) + \"_\" + new_df[\"hdbscan_cluster\"].astype(str)\n",
    "\n",
    "pca_result = pd.DataFrame(reduce_data(new_df, 2), columns=[\"PCA1\", \"PCA2\"])\n",
    "pca_result[\"combined_cluster\"] = new_df[\"combined_cluster\"]\n",
    "\n",
    "fig = px.scatter(pca_result, x=\"PCA1\", y=\"PCA2\", color=\"combined_cluster\", title=\"PCA Clustering\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for cluster, data in clustered_data.groupby(\"kmeans_cluster\"):\n",
    "    df = pd.DataFrame(reduce_data(data, 2), columns=[\"PCA1\", \"PCA2\"])\n",
    "    df[\"cluster\"] = data[\"hdbscan_cluster\"].reset_index(drop=True)\n",
    "\n",
    "    plt.scatter(df[\"PCA1\"], df[\"PCA2\"], c=df[\"cluster\"], cmap='viridis')   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming 'clustered_data' is your DataFrame and it has columns 'kmeans_cluster', 'hdbscan_cluster', 'x', and 'y'\n",
    "reduced_data = reduce_data(clustered_data, 2)\n",
    "\n",
    "reduced_data = pd.DataFrame(reduced_data, columns=['x', 'y'])\n",
    "\n",
    "reduced_data['kmeans_cluster'] = clustered_data['kmeans_cluster']\n",
    "reduced_data['hdbscan_cluster'] = clustered_data['hdbscan_cluster']\n",
    "\n",
    "display(reduced_data)\n",
    "# Get unique KMeans clusters\n",
    "kmeans_clusters = clustered_data['kmeans_cluster'].unique()\n",
    "\n",
    "# Define a color palette for HDBSCAN clusters, ensuring -1 is black\n",
    "hdbscan_clusters = clustered_data['hdbscan_cluster'].unique()\n",
    "palette = sns.color_palette('husl', len(hdbscan_clusters))\n",
    "palette_dict = {cluster: palette[i] for i, cluster in enumerate(hdbscan_clusters)}\n",
    "palette_dict[-1] = 'black'  # Ensure that -1 is black\n",
    "\n",
    "# Create a plot for each KMeans cluster\n",
    "for k_cluster in kmeans_clusters:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Filter the data for the current KMeans cluster\n",
    "    df_filtered = clustered_data[clustered_data['kmeans_cluster'] == k_cluster]\n",
    "    \n",
    "    # Plot each HDBSCAN cluster with a different color\n",
    "    for h_cluster in df_filtered['hdbscan_cluster'].unique():\n",
    "        cluster_data = df_filtered[df_filtered['hdbscan_cluster'] == h_cluster]\n",
    "        plt.scatter(cluster_data['x'], cluster_data['x'], \n",
    "                    color=palette_dict[h_cluster], label=f'HDBSCAN {h_cluster}')\n",
    "    \n",
    "    plt.title(f'KMeans Cluster {k_cluster}')\n",
    "    plt.xlabel('X Coordinate')\n",
    "    plt.ylabel('Y Coordinate')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import process, utils\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import hdbscan\n",
    "import joblib\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyOAuth\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "class MusicRecommendation:\n",
    "    def __init__(self, original_data: pd.DataFrame, weight_year: float, weight_popularity: float):\n",
    "\n",
    "        load_dotenv()\n",
    "\n",
    "        # Initialize the Spotify client with authentication\n",
    "        auth_manager = SpotifyOAuth(client_id=os.getenv('SPOTIFY_CLIENT_ID'), client_secret=os.getenv('SPOTIFY_CLIENT_SECRET'), redirect_uri=os.getenv('SPOTIFY_REDIRECT_URI'), scope=\"user-library-read\")\n",
    "        self.sp = spotipy.Spotify(auth_manager=auth_manager)\n",
    "        self.original_data = original_data\n",
    "        self.weight_year = weight_year\n",
    "        self.weight_popularity = weight_popularity\n",
    "        self.clustering_method = \"kmeans\"\n",
    "        self.hdbscan_model = joblib.load(\"hdbscan_model.pkl\")\n",
    "        self.kmeans_model = joblib.load(\"kmeans_model.pkl\")\n",
    "        self.clustered_data = pd.read_parquet(\"clustered_subset.parquet\") if self.clustering_method == \"hdbscan\" else load_and_concatenate_parquet_files(\"data/kmean_clustered_subset\")\n",
    "        self.metrics = [\n",
    "            'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', \n",
    "            'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', \n",
    "            'time_signature'\n",
    "        ]\n",
    "    \n",
    "    def get_track_features(self, song_name: str, artist_name: str) -> pd.DataFrame:\n",
    "        results = self.sp.search(q=f\"track:{song_name} artist:{artist_name}\", type=\"track\", limit=1)\n",
    "        \n",
    "        if not results['tracks']['items']:\n",
    "            print(\"Track not found\")\n",
    "            return pd.DataFrame()  # Return an empty DataFrame if the track is not found\n",
    "        \n",
    "        track = results['tracks']['items'][0]\n",
    "        track_id = track['id']\n",
    "        track_name = track['name']\n",
    "        artist_name = track['artists'][0]['name']\n",
    "        popularity = track['popularity']\n",
    "        release_date = track['album']['release_date']\n",
    "        year = int(release_date.split('-')[0])\n",
    "        duration_ms = track['duration_ms']\n",
    "        audio_features = self.sp.audio_features(track_id)[0]\n",
    "        \n",
    "        data = {\n",
    "            'danceability': audio_features['danceability'],\n",
    "            'energy': audio_features['energy'],\n",
    "            'key': audio_features['key'],\n",
    "            'loudness': audio_features['loudness'],\n",
    "            'mode': audio_features['mode'],\n",
    "            'speechiness': audio_features['speechiness'],\n",
    "            'acousticness': audio_features['acousticness'],\n",
    "            'instrumentalness': audio_features['instrumentalness'],\n",
    "            'liveness': audio_features['liveness'],\n",
    "            'valence': audio_features['valence'],\n",
    "            'tempo': audio_features['tempo'],\n",
    "            'time_signature': audio_features['time_signature'],\n",
    "            'artist_name': artist_name,\n",
    "            'track_name': track_name,\n",
    "            'track_id': track_id,\n",
    "            'popularity': popularity,\n",
    "            'year': year,\n",
    "            'duration_ms': duration_ms\n",
    "        }\n",
    "        return pd.DataFrame([data])\n",
    "\n",
    "    def normalized_data(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        min_max_scaler = MinMaxScaler()\n",
    "        numeric_columns = data[self.metrics]\n",
    "        other_columns = data.drop(columns=self.metrics).reset_index(drop=True)\n",
    "        normalized_data = min_max_scaler.fit_transform(numeric_columns)\n",
    "        return pd.merge(pd.DataFrame(normalized_data, columns=self.metrics), other_columns, left_index=True, right_index=True)\n",
    "    \n",
    "    def reduce_data(self, data: pd.DataFrame, dimensions: int) -> np.ndarray:\n",
    "        numeric_columns = data[self.metrics]\n",
    "        pca = PCA(n_components=dimensions)\n",
    "        return pca.fit_transform(numeric_columns)\n",
    "\n",
    "    def get_closest_match(self, user_input: str, df: pd.DataFrame, column: str, threshold: int = 92) -> str:\n",
    "        processed_user_input = utils.default_process(user_input)\n",
    "        strings_column = df[column].dropna()\n",
    "        processed_strings = [utils.default_process(string) for string in strings_column]\n",
    "        match = process.extractOne(processed_user_input, processed_strings, processor=None, score_cutoff=threshold)\n",
    "        return strings_column.iloc[match[2]] if match else None\n",
    "\n",
    "    def song_finder(self, song_name: str, artist_name: str, data) -> pd.DataFrame:\n",
    "        song = data[(data[\"track_name\"] == song_name) & (data[\"artist_name\"] == artist_name)]\n",
    "        return song if not song.empty else None\n",
    "\n",
    "    def preprocess_song(self, song: pd.DataFrame, normalization: str, reduction: str) -> pd.DataFrame:\n",
    "        if normalization == \"standardized\":\n",
    "            song = self.standardized_data(song)\n",
    "        elif normalization == \"normalized\":\n",
    "            song = self.normalized_data(song)\n",
    "        if reduction == \"pca\":\n",
    "            song = self.reduce_data(song, 2)\n",
    "        return song\n",
    "\n",
    "    def get_song_cluster(self, song: pd.DataFrame) -> int:\n",
    "        if self.clustering_method == \"hdbscan\":\n",
    "            new_data_point = song[self.metrics].values.reshape(1, -1)\n",
    "            predicted_cluster, _ = hdbscan.approximate_predict(self.hdbscan_model, new_data_point)\n",
    "            cluster = predicted_cluster[0]\n",
    "            \n",
    "        elif self.clustering_method == \"kmeans\":\n",
    "            cluster = self.kmeans_model.predict(song[self.metrics])[0]\n",
    "        return cluster\n",
    "\n",
    "    def find_nearest_neighbors(self, song: pd.DataFrame, data: pd.DataFrame, number_of_songs: int) -> tuple:\n",
    "        knn_model = NearestNeighbors(n_neighbors=100)\n",
    "        cluster_data = data[self.metrics]\n",
    "        knn_model.fit(cluster_data)\n",
    "        distances, indices = knn_model.kneighbors(song[self.metrics], n_neighbors=100)\n",
    "        neighbors_df = data.iloc[indices[0]]\n",
    "        return neighbors_df, distances[0]\n",
    "\n",
    "    def get_weighted_scores(self, neighbors_df: pd.DataFrame, neighbor_distances: np.ndarray) -> pd.DataFrame:\n",
    "        # Ensure we are working with a copy of the DataFrame to avoid SettingWithCopyWarning\n",
    "        neighbors_df = neighbors_df.copy()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Convert 'year' column to numeric\n",
    "        neighbors_df['year'] = pd.to_datetime(neighbors_df['year'], format='%Y')\n",
    "        neighbors_df['year_numeric'] = neighbors_df['year'].dt.year\n",
    "        \n",
    "        # Normalize 'year_numeric' and 'popularity' columns\n",
    "        scaler = MinMaxScaler()\n",
    "        neighbors_df[['year_normalized', 'popularity_normalized']] = scaler.fit_transform(\n",
    "            neighbors_df[['year_numeric', 'popularity']]\n",
    "        )\n",
    "        \n",
    "        year_normalized = neighbors_df['year_normalized'].values\n",
    "        popularity_normalized = neighbors_df['popularity_normalized'].values\n",
    "        \n",
    "        # Define weights for year and popularity\n",
    "        year_weight = self.weight_year\n",
    "        popularity_weight = self.weight_popularity\n",
    "\n",
    "        # Compute base scores (inverse distance, to ensure higher similarity has a higher base score)\n",
    "        base_scores = 1 / (neighbor_distances + 1e-8)  # Avoid division by zero\n",
    "\n",
    "        # Compute boosting scores\n",
    "        boosting_scores = year_normalized * year_weight + popularity_normalized * popularity_weight\n",
    "\n",
    "        # Final scores: add boosting scores to base scores\n",
    "        final_scores = base_scores + boosting_scores\n",
    "\n",
    "        # Rank neighbors based on weighted scores\n",
    "        ranked_indices = np.argsort(final_scores)[::-1]  # Sort in descending order\n",
    "        return neighbors_df.iloc[ranked_indices]\n",
    "\n",
    "    def print_preview_urls(self, song_df: pd.DataFrame) -> None:\n",
    "        for _, row in song_df.iterrows():\n",
    "            track_id = row['track_id']\n",
    "            track = self.sp.track(track_id)\n",
    "            preview_url = track.get('preview_url')\n",
    "            if preview_url:\n",
    "                print(f\"Track: {row['track_name']} by {row['artist_name']}\")\n",
    "                print(f\"Preview URL: {preview_url}\")\n",
    "            else:\n",
    "                print(f\"Track: {row['track_name']} by {row['artist_name']}\")\n",
    "                print(\"Preview URL not available.\")\n",
    "    \n",
    "    def choose_correct_song(self, song: pd.DataFrame) -> pd.DataFrame:\n",
    "        if len(song) > 1:\n",
    "            chosen_index = int(input(\"Enter the index of the song you want to use: \"))\n",
    "            if chosen_index >= len(song):\n",
    "                print(\"Invalid index, using 1st song\")\n",
    "                chosen_index = 0\n",
    "            song = song.iloc[chosen_index].copy()\n",
    "        return song\n",
    "    \n",
    "    def remove_song_if_present(self, song: pd.DataFrame, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        matched_rows = data[(data[\"track_name\"] == song[\"track_name\"].values[0]) & (data[\"artist_name\"] == song[\"artist_name\"].values[0])]\n",
    "        if not matched_rows.empty:\n",
    "            print(f\"Removing {len(matched_rows)} rows from possible recommendations\")\n",
    "            if len(matched_rows) > 1:\n",
    "                print(\"Rows to be removed:\")\n",
    "                display(matched_rows.head(5))\n",
    "            indexes_to_remove = matched_rows.index\n",
    "            return data.drop(indexes_to_remove)\n",
    "        else:\n",
    "            return data\n",
    "    \n",
    "    \n",
    "    def find_closest_songs(self, song_name: str = \"\", artist_name: str = \"\", same_artist: bool = \"\", number_of_songs: int = \"\", weighted_system: bool = \"\") -> pd.DataFrame:\n",
    "        if artist_name == \"\":\n",
    "            artist_name = input(\"Enter the artist name: \")\n",
    "        if song_name == \"\":\n",
    "            song_name = input(\"Enter the song name: \")\n",
    "        if same_artist == \"\":\n",
    "            same_artist = input(\"Filter by same artist? (yes/no): \").strip().lower() == 'yes'\n",
    "        if number_of_songs == \"\":\n",
    "            number_of_songs = int(input(\"Enter the number of songs to return: \"))\n",
    "        if weighted_system == \"\":\n",
    "            weighted_system = input(\"Use weighted system? (yes/no): \").strip().lower() == 'yes'\n",
    "\n",
    "        song = self.get_track_features(song_name, artist_name)\n",
    "        \n",
    "        if song is None:\n",
    "            print(\"No match found\")\n",
    "            return None\n",
    "        \n",
    "        print(\"Song found\")\n",
    "        real_artist_name = song[\"artist_name\"].values[0]\n",
    "        real_song_name = song[\"track_name\"].values[0]\n",
    "        print(f\"Using {real_song_name} by {real_artist_name}\")\n",
    "        display(song.reset_index(drop=True))\n",
    "        self.print_preview_urls(song)\n",
    "        \n",
    "        song = self.choose_correct_song(song)\n",
    "        \n",
    "        clustered_data = self.remove_song_if_present(song, self.clustered_data)\n",
    "        \n",
    "        all_songs = pd.concat([clustered_data, song], ignore_index=True) \n",
    "        \n",
    "        all_song_standardized = self.normalized_data(all_songs)\n",
    "        song_standardized = self.song_finder(real_song_name, real_artist_name, all_song_standardized)\n",
    "        \n",
    "        predicted_cluster = self.get_song_cluster(song_standardized)\n",
    "        \n",
    "        print(f\"\\nPredicted cluster: {predicted_cluster}\\n\")\n",
    "        \n",
    "        sample_data = all_song_standardized[all_song_standardized[\"cluster\"] == predicted_cluster]\n",
    "        if not same_artist:\n",
    "            sample_data = sample_data[sample_data[\"artist_name\"] != artist_name]\n",
    "         \n",
    "        neighbors_df, neighbor_distances = self.find_nearest_neighbors(song_standardized, sample_data, number_of_songs)\n",
    "        \n",
    "        if weighted_system:\n",
    "            neighbors_df = self.get_weighted_scores(neighbors_df, neighbor_distances)\n",
    "        \n",
    "        closest_songs = self.original_data.loc[neighbors_df.head(number_of_songs).index]\n",
    "        \n",
    "        closest_songs = closest_songs[song.columns]\n",
    "        \n",
    "        self.print_preview_urls(closest_songs)\n",
    "        \n",
    "        return closest_songs\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize with default paths and parameters\n",
    "   \n",
    "    recommender = MusicRecommendation(original_data = original_data, weight_year=0.6, weight_popularity=0.4)\n",
    "\n",
    "    # Find closest songs with\n",
    "\n",
    "recommender.find_closest_songs(artist_name=\"Skrillex\", song_name=\"bangarang\", same_artist=True, number_of_songs=5, weighted_system=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here the modelling and transformation starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Select the numeric columns\n",
    "numeric_columns = feature_df.drop(columns=[\"track_id\", \"genre\"])\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# Standardize the numeric columns\n",
    "standardized_data = standard_scaler.fit_transform(numeric_columns)\n",
    "standardized_df = pd.DataFrame(standardized_data, columns=numeric_columns.columns)\n",
    "standardized_df['genre'] = feature_df['genre']\n",
    "standardized_df['track_id'] = feature_df['track_id']\n",
    "\n",
    "# Normalize the numeric columns\n",
    "normalized_data = min_max_scaler.fit_transform(numeric_columns)\n",
    "normalized_df = pd.DataFrame(normalized_data, columns=numeric_columns.columns)\n",
    "normalized_df['genre'] = feature_df['genre']\n",
    "normalized_df['track_id'] = feature_df['track_id']\n",
    "\n",
    "# Display the standardized and normalized dataframes\n",
    "display(standardized_df.describe())\n",
    "display(normalized_df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(standardized_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "# Perform PCA on standardized_data\n",
    "pca_standardized = PCA(n_components=2)\n",
    "pca_standardized_result = pca_standardized.fit_transform(standardized_data)\n",
    "print(1)\n",
    "\n",
    "# Perform PCA on normalized_data\n",
    "pca_normalized = PCA(n_components=2)\n",
    "pca_normalized_result = pca_normalized.fit_transform(normalized_data)\n",
    "print(2)\n",
    "\n",
    "# # Perform t-SNE on standardized_data\n",
    "# tsne_standardized = TSNE(n_components=2)\n",
    "# tsne_standardized_result = tsne_standardized.fit_transform(standardized_data)\n",
    "# print(3)\n",
    "\n",
    "# # Perform t-SNE on normalized_data\n",
    "# tsne_normalized = TSNE(n_components=2)\n",
    "# tsne_normalized_result = tsne_normalized.fit_transform(normalized_data)\n",
    "# print(4)\n",
    "\n",
    "# # Create the subplot with 4 plots\n",
    "# fig = px.subplots(\n",
    "#     rows=2, cols=2,\n",
    "#     subplot_titles=(\"PCA - Standardized Data\", \"PCA - Normalized Data\", \"t-SNE - Standardized Data\", \"t-SNE - Normalized Data\"),\n",
    "#     shared_xaxes=True, shared_yaxes=True,\n",
    "#     vertical_spacing=0.1, horizontal_spacing=0.1\n",
    "# )\n",
    "\n",
    "# # Add PCA - Standardized Data plot\n",
    "# fig.add_trace(\n",
    "#     px.scatter(x=pca_standardized_result[:, 0], y=pca_standardized_result[:, 1], color=standardized_df['track_genre']).data[0],\n",
    "#     row=1, col=1\n",
    "# )\n",
    "\n",
    "# # Add PCA - Normalized Data plot\n",
    "# fig.add_trace(\n",
    "#     px.scatter(x=pca_normalized_result[:, 0], y=pca_normalized_result[:, 1], color=normalized_df['track_genre']).data[0],\n",
    "#     row=1, col=2\n",
    "# )\n",
    "\n",
    "# # Add t-SNE - Standardized Data plot\n",
    "# fig.add_trace(\n",
    "#     px.scatter(x=tsne_standardized_result[:, 0], y=tsne_standardized_result[:, 1], color=standardized_df['track_genre']).data[0],\n",
    "#     row=2, col=1\n",
    "# )\n",
    "\n",
    "# # Add t-SNE - Normalized Data plot\n",
    "# fig.add_trace(\n",
    "#     px.scatter(x=tsne_normalized_result[:, 0], y=tsne_normalized_result[:, 1], color=normalized_df['track_genre']).data[0],\n",
    "#     row=2, col=2\n",
    "# )\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(\n",
    "#     height=800,\n",
    "#     showlegend=False\n",
    "# )\n",
    "\n",
    "# # Show the subplot\n",
    "# fig.show()\n",
    "\n",
    "px.scatter(x=pca_standardized_result[:, 0], y=pca_standardized_result[:, 1], color=standardized_df['genre']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(original_data, hue='track_genre', diag_kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# import plotly.express as px\n",
    "\n",
    "# dataframe = standardized_df.copy()\n",
    "# # Assuming 'data' is your dataframe and track_genre is a column in the dataframe\n",
    "\n",
    "# # Create a subset of the data\n",
    "# subset_data = dataframe.sample(n=1000, random_state=42)\n",
    "\n",
    "# # Prepare the data: Separate features and labels\n",
    "# features = subset_data.drop(columns=['track_genre', \"track_id\"])  # Drop the track_genre column\n",
    "# labels = subset_data['track_genre']  # Save the track_genre column separately\n",
    "\n",
    "# # Apply t-SNE\n",
    "# tsne = TSNE(n_components=2, random_state=42)\n",
    "# tsne_results = tsne.fit_transform(features)\n",
    "\n",
    "# # Create a DataFrame for the t-SNE results\n",
    "# tsne_df = pd.DataFrame(tsne_results, columns=['tsne_1', 'tsne_2'])\n",
    "# tsne_df['track_genre'] = labels.values\n",
    "\n",
    "# # Plot the results using Plotly Express\n",
    "# fig = px.scatter(tsne_df, x='tsne_1', y='tsne_2', color='track_genre', title='t-SNE of Track Features by Genre')\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import umap\n",
    "\n",
    "# reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "\n",
    "# # Apply UMAP\n",
    "# umap_results = reducer.fit_transform(subset_data.drop(columns=['track_genre', \"track_id\"]))\n",
    "\n",
    "# px.scatter(x=umap_results[:, 0], y=umap_results[:, 1], color=subset_data['track_genre']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from datetime import datetime\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # a function to get content-based recommendations based on music features\n",
    "# def content_based_recommendations(input_song_name, num_recommendations=5):\n",
    "#     if input_song_name not in music_df['Track Name'].values:\n",
    "#         print(f\"'{input_song_name}' not found in the dataset. Please enter a valid song name.\")\n",
    "#         return\n",
    "\n",
    "#     # Get the index of the input song in the music DataFrame\n",
    "#     input_song_index = music_df[music_df['Track Name'] == input_song_name].index[0]\n",
    "\n",
    "#     # Calculate the similarity scores based on music features (cosine similarity)\n",
    "#     similarity_scores = cosine_similarity([music_features_scaled[input_song_index]], music_features_scaled)\n",
    "\n",
    "#     # Get the indices of the most similar songs\n",
    "#     similar_song_indices = similarity_scores.argsort()[0][::-1][1:num_recommendations + 1]\n",
    "\n",
    "#     # Get the names of the most similar songs based on content-based filtering\n",
    "#     content_based_recommendations = music_df.iloc[similar_song_indices][['Track Name', 'Artists', 'Album Name', 'Release Date', 'Popularity']]\n",
    "\n",
    "#     return content_based_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Loop through a range of cluster numbers to calculate silhouette scores\n",
    "silhouette_scores = []\n",
    "cluster_range = range(2, 26)\n",
    "data_sample = standardized_data[np.random.choice(standardized_data.shape[0], 20000, replace=False)]\n",
    "for k in cluster_range:\n",
    "    print(f\"Calculating silhouette score for k = {k}\")\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(data_sample)\n",
    "    silhouette_avg = silhouette_score(data_sample, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "    print(f\"For n_clusters = {k}, the average silhouette score is {silhouette_avg:.4f}\")\n",
    "\n",
    "# Optionally, you can plot the silhouette scores\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(cluster_range, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Scores for k-means clustering')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "data_sample = standardized_data[np.random.choice(standardized_data.shape[0], 200000, replace=False)]\n",
    "\n",
    "# Fit the HDBSCAN model\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=100)\n",
    "hdbscan_model.fit(data_sample)\n",
    "\n",
    "# Get the labels assigned to each data point\n",
    "cluster_labels = hdbscan_model.labels_\n",
    "\n",
    "# Example: Print out the first 10 cluster labels\n",
    "print(\"First 10 cluster labels:\", cluster_labels[:10])\n",
    "\n",
    "# Print out the number of clusters found (excluding noise)\n",
    "print(f\"Number of clusters found: {len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def song_finder(song_name, artist_name):\n",
    "    song = clustered_data[(clustered_data[\"track_name\"] == song_name) & (clustered_data[\"artist_name\"] == artist_name)]\n",
    "    return song\n",
    "\n",
    "song = song_finder(\"Shape of You\", \"Ed Sheeran\")\n",
    "\n",
    "standardized_data[song.index]\n",
    "\n",
    "for song in clustered_data[['track_name', 'artist_name']].itertuples():\n",
    "    print(song[0])\n",
    "    print(standardized_data[song[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def song_finder(song_name, artist_name):\n",
    "    song = clustered_data[(clustered_data[\"track_name\"] == song_name) & (clustered_data[\"artist_name\"] == artist_name)]\n",
    "    return song\n",
    "\n",
    "def find_closest_songs(song_name, artist_name, song_number=5):\n",
    "    all_distances = []\n",
    "    \n",
    "    chosen_song = song_finder(song_name, artist_name)\n",
    "    index = chosen_song.index\n",
    "    print(index)\n",
    "    print(standardized_data[index][0])\n",
    "    for song in clustered_data[['track_name', 'artist_name']].itertuples():\n",
    "\n",
    "        current_distance = distance.cosine(standardized_data[song[0]],standardized_data[chosen_song.index][0])\n",
    "        all_distances.append((song.track_name, song.artist_name, current_distance))\n",
    "    all_distances.sort(key=lambda x: x[2], reverse=False)\n",
    "    return all_distances[1:song_number+1]\n",
    "\n",
    "find_closest_songs(\"Shape of You\", \"Skrillex\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

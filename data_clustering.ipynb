{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_and_concatenate_parquet_files(folder_path):\n",
    "    # List all files in the directory\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith('.parquet')]\n",
    "    \n",
    "    # Sort files for consistent order if needed (optional)\n",
    "    files.sort()\n",
    "\n",
    "    # Load and concatenate all the Parquet files\n",
    "    df_list = []\n",
    "    for file in files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_parquet(file_path)\n",
    "        df_list.append(df)\n",
    "\n",
    "    # Concatenate all dataframes\n",
    "    concatenated_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    return concatenated_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "def save_dataframe_as_parquet(df:pd.DataFrame, folder_path=\"data\", folder_name=None, always_overwrite=None, model_object=None):\n",
    "    if not folder_name:\n",
    "        folder_name = input(\"Enter the name of the folder to save the files: \")\n",
    "\n",
    "    full_path = os.path.join(folder_path, folder_name)\n",
    "\n",
    "    # Check if the folder already exists\n",
    "    if os.path.exists(full_path) and always_overwrite is not True:\n",
    "        if always_overwrite is None:\n",
    "            overwrite = input(f\"The folder '{folder_name}' already exists. Do you want to overwrite it? (yes/no): \")\n",
    "            always_overwrite = overwrite.lower() != 'yes'\n",
    "        if not always_overwrite:\n",
    "            suffix = 1\n",
    "            new_folder_name = f\"{folder_name}_{suffix}\"\n",
    "            while os.path.exists(os.path.join(folder_path, new_folder_name)):\n",
    "                suffix += 1\n",
    "                new_folder_name = f\"{folder_name}_{suffix}\"\n",
    "            folder_name = new_folder_name\n",
    "            full_path = os.path.join(folder_path, folder_name)\n",
    "    \n",
    "    os.makedirs(full_path, exist_ok=True)\n",
    "\n",
    "    temp_file = os.path.join(full_path, \"temp.parquet\")\n",
    "    df.to_parquet(temp_file)\n",
    "    file_size = os.path.getsize(temp_file) / (1024 * 1024)  # Size in MB\n",
    "    os.remove(temp_file)\n",
    "\n",
    "    if file_size > 50:\n",
    "        num_splits = math.ceil(file_size / 50)\n",
    "        row_split = math.ceil(len(df) / num_splits)\n",
    "    else:\n",
    "        num_splits = 1\n",
    "        row_split = len(df)\n",
    "    \n",
    "    for i in range(num_splits):\n",
    "        start_row = i * row_split\n",
    "        end_row = min((i + 1) * row_split, len(df))\n",
    "        split_df = df.iloc[start_row:end_row]\n",
    "        split_file_name = os.path.join(full_path, f\"{folder_name}_part_{i + 1}.parquet\")\n",
    "        split_df.to_parquet(split_file_name)\n",
    "    \n",
    "    if model_object:\n",
    "        for key, value in model_object.items():\n",
    "            joblib.dump(value, f'{full_path}/{key}_model.pkl')\n",
    "\n",
    "    print(f\"Dataframe saved in {num_splits} files under the folder: {full_path}\")\n",
    "    \n",
    "    return full_path\n",
    "\n",
    "# Example usage:\n",
    "# save_dataframe_as_parquet(df=kmeans_cluster, folder_path=\"data\", folder_name=\"kmeans_clustered_subset\", always_overwrite=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import hdbscan\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def reduce_data(data, dimensions, metric_columns):\n",
    "    numeric_columns = data[metric_columns]\n",
    "    pca_standardized = PCA(n_components=dimensions)\n",
    "    pca_standardized_result = pca_standardized.fit_transform(numeric_columns)\n",
    "    return pca_standardized_result\n",
    "\n",
    "def normalize_data(data:pd.DataFrame, metric_columns):\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    numeric_columns = data[metric_columns]\n",
    "    other_columns = data.drop(columns=metric_columns).reset_index(drop=True)\n",
    "    normalized_data = min_max_scaler.fit_transform(numeric_columns)\n",
    "    normalized_df = pd.DataFrame(normalized_data, columns=numeric_columns.columns)\n",
    "    normalized_data = pd.merge(normalized_df, other_columns, left_index=True, right_index=True)\n",
    "    return normalized_df\n",
    "\n",
    "def cluster_with_kmeans(data_for_clustering:pd.DataFrame):\n",
    "    kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "    kmeans_labels = kmeans.fit_predict(data_for_clustering)\n",
    "    return kmeans_labels, kmeans\n",
    "\n",
    "def cluster_with_hdbscan(data_for_clustering:pd.DataFrame):\n",
    "    hdbscan_clusterer = hdbscan.HDBSCAN(min_cluster_size=10, prediction_data=True)\n",
    "    hdbscan_labels = hdbscan_clusterer.fit_predict(data_for_clustering)\n",
    "    return hdbscan_labels, hdbscan_clusterer\n",
    "\n",
    "def run_kmeans_and_hdbscan(original_data:pd.DataFrame, cluster_data:pd.DataFrame):\n",
    "    data_for_clustering = cluster_data.copy()\n",
    "    kmeans_labels, kmeans = cluster_with_kmeans(data_for_clustering)\n",
    "\n",
    "    data_for_clustering[\"kmeans_cluster\"] = kmeans_labels\n",
    "    original_data[\"kmeans_cluster\"] = kmeans_labels\n",
    "    print(f\"KMeans_cluster distro: {data_for_clustering['kmeans_cluster'].value_counts()}\")\n",
    "    hdbscan_cluster_df = []\n",
    "    hdbscan_models = {}\n",
    "    for cluster, data in data_for_clustering.groupby(\"kmeans_cluster\"):\n",
    "        hdbscan_cluster_labels, hdbscan_clusterer = cluster_with_hdbscan(data.drop(columns=[\"kmeans_cluster\"]))\n",
    "        current_original_data = original_data[original_data[\"kmeans_cluster\"] == cluster].reset_index(drop=True).copy()\n",
    "        current_original_data[\"hdbscan_cluster\"] = hdbscan_cluster_labels\n",
    "        hdbscan_cluster_df.append(current_original_data)\n",
    "        hdbscan_models[cluster] = hdbscan_clusterer\n",
    "        print(f\"HDBSCAN_cluster distro for kmeans {cluster}: {current_original_data['hdbscan_cluster'].value_counts()}\")\n",
    "\n",
    "    all_cluster_results = pd.concat(hdbscan_cluster_df).reset_index(drop=True)\n",
    "    \n",
    "    return all_cluster_results, kmeans, hdbscan_models\n",
    "\n",
    "def cluster_data(original_data:pd.DataFrame, subset_fraction=None, kmeans=True, use_hdbscan=True, should_normalize_data=True, pca_dimensions=None, columns_to_use=None):\n",
    "    \n",
    "    if subset_fraction:\n",
    "        original_data = original_data.sample(frac=subset_fraction)\n",
    "    \n",
    "    data_for_clustering = original_data.copy().reset_index(drop=True)\n",
    "    \n",
    "    metric_columns = columns_to_use if columns_to_use else [\"danceability\", \"energy\", \"key\", \"loudness\", \"mode\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\", \"time_signature\"]\n",
    "    \n",
    "    if should_normalize_data:\n",
    "        data_for_clustering = normalize_data(data_for_clustering, metric_columns=metric_columns)\n",
    "\n",
    "    if pca_dimensions:\n",
    "        data_for_clustering = reduce_data(data_for_clustering, pca_dimensions)\n",
    "    \n",
    "    if pca_dimensions is None:\n",
    "        data_for_clustering = data_for_clustering[metric_columns]\n",
    "\n",
    "    if kmeans and use_hdbscan:\n",
    "        result_df, kmeans, hdbscan_models = run_kmeans_and_hdbscan(original_data, data_for_clustering)\n",
    "        folder_path = save_dataframe_as_parquet(result_df, folder_path=\"data\", folder_name=\"kmeans_hdbscan_clustered_subset\", always_overwrite=False, model_object={\"kmeans\": kmeans})\n",
    "        for key, value in hdbscan_models.items():\n",
    "            joblib.dump(value, f'{folder_path}/hdbscan_model_{key}.pkl')\n",
    "        result = result_df.copy()\n",
    "    \n",
    "    elif kmeans:\n",
    "        kmeans_labels, kmeans = cluster_with_kmeans(data_for_clustering)\n",
    "        original_data[\"kmeans_cluster\"] = kmeans_labels\n",
    "        save_dataframe_as_parquet(original_data, folder_path=\"data\", folder_name=\"kmeans_clustered_subset\", always_overwrite=False, model_object={\"kmeans\": kmeans})\n",
    "        print(f\"KMeans_cluster distro: {original_data['kmeans_cluster'].value_counts()}\")\n",
    "        result = original_data.copy()\n",
    "        \n",
    "    elif use_hdbscan:\n",
    "        hdbscan_labels, hdbscan_clusterer = cluster_with_hdbscan(data_for_clustering)\n",
    "        original_data[\"hdbscan_cluster\"] = hdbscan_labels\n",
    "        save_dataframe_as_parquet(original_data, folder_path=\"data\", folder_name=\"hdbscan_clustered_subset\", always_overwrite=False, model_object={\"hdbscan\": hdbscan_clusterer})\n",
    "        print(f\"HDBSCAN_cluster distro: {original_data['hdbscan_cluster'].value_counts()}\")\n",
    "        result = original_data.copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "preprocessed_data = load_and_concatenate_parquet_files(\"data/preprocessed_spotify_data\")\n",
    "clustered_data = cluster_data(preprocessed_data, subset_fraction=0.02, kmeans=False, use_hdbscan=True,  should_normalize_data=True, pca_dimensions=None, columns_to_use=None)\n",
    "display(clustered_data)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
